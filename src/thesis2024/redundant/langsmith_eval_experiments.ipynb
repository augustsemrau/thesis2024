{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "from langchain import chat_models, prompts, smith\n",
    "from langchain.schema import output_parser\n",
    "\n",
    "from langsmith.evaluation import evaluate\n",
    "from langsmith.schemas import Run, Example\n",
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "from thesis2024.utils import init_llm_langsmith\n",
    "\n",
    "import uuid\n",
    "\n",
    "\n",
    "langsmith_name = \"Langsmith Eval Experiment 1\"\n",
    "_ = init_llm_langsmith(llm_key=3, temp=0.5, langsmith_name=langsmith_name)\n",
    "\n",
    "\n",
    "uid = uuid.uuid4()\n",
    "client = langsmith.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prompt for the Teaching Agent System.\"\"\"\n",
    "def build_tas_prompt(self):\n",
    "    \"\"\"Build the agent prompt.\"\"\"\n",
    "    system_message = \"\"\"You will interact with a student who has no prior knowledge of the subject.\"\"\"\n",
    "    course = \"\"\"Mathematics 1.\"\"\"\n",
    "    subject = \"\"\"All subjects.\"\"\"\n",
    "\n",
    "    prompt_hub_template = hub.pull(\"augustsemrau/react-teaching-chat\").template\n",
    "    prompt_template = PromptTemplate.from_template(template=prompt_hub_template)\n",
    "    prompt = prompt_template.partial(system_message=system_message, course_name=course, subject_name=subject)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-dbrx-qa-custom-eval-is-answered-0f7f01bb' at:\n",
      "https://smith.langchain.com/o/f6c953c1-69e2-5115-bc32-1c4edca6df0d/datasets/5672906f-7370-47d4-a6be-b959bb88caf2/compare?selectedSessions=aad5c241-40a0-4033-9b16-ce1c6b2b50fe\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cf2b91a32f4e22bf9ca0a789c6959f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'human', 'content': 'Hello, I am August!', 'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, {'type': 'ai', 'content': 'Hello August! How can I assist you today with Mathematics 1?', 'example': False, 'tool_calls': [], 'additional_kwargs': {}, 'response_metadata': {}, 'invalid_tool_calls': []}, {'type': 'human', 'content': 'Can you explain me how ADAM optimization works?', 'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, {'type': 'ai', 'content': 'I can explain how ADAM optimization works. ADAM (Adaptive Moment Estimation) is an optimization algorithm that is used to update the parameters of a neural network based on the gradients of the loss function. It combines the advantages of two other optimization algorithms, RMSprop and AdaGrad. ADAM uses adaptive learning rates for each parameter, which helps converge faster and handle sparse gradients. The algorithm computes the first and second moments of the gradients and uses them to update the parameters. This allows ADAM to adjust the learning rate for each parameter individually, making it well-suited for training deep neural networks.', 'example': False, 'tool_calls': [], 'additional_kwargs': {}, 'response_metadata': {}, 'invalid_tool_calls': []}, {'type': 'human', 'content': 'What is the name of the person who invented this optimization technique?', 'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, {'type': 'ai', 'content': 'The optimization technique ADAM (Adaptive Moment Estimation) was invented by Diederik Kingma and Jimmy Ba.', 'example': False, 'tool_calls': [], 'additional_kwargs': {}, 'response_metadata': {}, 'invalid_tool_calls': []}]\n",
      "[{'type': 'human', 'content': 'Hello, I am August!', 'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, {'type': 'ai', 'content': 'Hello August! How can I assist you today with your studies in computer science? If you have questions about Gradient Descent or any other topic, feel free to ask!', 'example': False, 'tool_calls': [], 'additional_kwargs': {}, 'response_metadata': {}, 'invalid_tool_calls': []}, {'type': 'human', 'content': 'Can you explain me how ADAM optimization works?', 'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, {'type': 'ai', 'content': \"Hello, August! I'd be happy to explain how Adam optimization works.\\n\\nAdam, short for Adaptive Moment Estimation, is an optimization algorithm that's used particularly in training neural networks. It combines ideas from two other extensions of stochastic gradient descent, specifically AdaGrad and RMSProp.\\n\\n1. **Adaptive Gradient Algorithm (AdaGrad)**: AdaGrad adapts the learning rates by scaling them inversely proportional to the square root of all past squared gradients. While this works well for sparse gradients, it may lead to an excessively rapid decrease in the learning rate for dense gradients (common in neural networks).\\n\\n2. **Root Mean Square Propagation (RMSProp)**: This algorithm modifies AdaGrad to alleviate its aggressive, monotonically decreasing learning rate. It does this by using a moving average of squared gradients to normalize the gradient itself. This adjustment helps to maintain a more suitable learning rate over time.\\n\\nHere’s how Adam essentially works step-by-step:\\n- Adam computes adaptive learning rates for each parameter. \\n- In addition to storing an exponentially decaying average of past squared gradients like RMSProp, Adam also keeps an exponentially decaying average of past gradients, similar to momentum.\\n- It calculates the corrections for first and second moments (the means and the uncentered variances of the gradients, respectively) to account for their initial bias toward zero. \\n- Combining these estimates, Adam adjusts the parameters to minimize the loss function of the neural network.\\n\\nAdam's advantages include relatively low memory requirements and being invariant to diagonal rescale of the gradients. Moreover, it is well suited for problems that are large in terms of data and/or parameters or when the gradients are sparse. Adam is also straightforward to implement and has been universally adopted in the deep learning community.\\n\\nI hope this helps you understand Adam optimizer! If you have more questions or need further clarification, feel free to ask.\", 'example': False, 'tool_calls': [], 'additional_kwargs': {}, 'response_metadata': {}, 'invalid_tool_calls': []}]\n",
      "Student: Hello, I am August!\n",
      "\n",
      "Teaching Assistant: Hello August! How can I assist you today with Mathematics 1?\n",
      "Student: Can you explain me how ADAM optimization works?\n",
      "\n",
      "Teaching Assistant: I can explain how ADAM optimization works. ADAM (Adaptive Moment Estimation) is an optimization algorithm that is used to update the parameters of a neural network based on the gradients of the loss function. It combines the advantages of two other optimization algorithms, RMSprop and AdaGrad. ADAM uses adaptive learning rates for each parameter, which helps converge faster and handle sparse gradients. The algorithm computes the first and second moments of the gradients and uses them to update the parameters. This allows ADAM to adjust the learning rate for each parameter individually, making it well-suited for training deep neural networks.\n",
      "Student: What is the name of the person who invented this optimization technique?\n",
      "\n",
      "Teaching Assistant: The optimization technique ADAM (Adaptive Moment Estimation) was invented by Diederik Kingma and Jimmy Ba.\n",
      "\n",
      "Student: Hello, I am August!\n",
      "\n",
      "Teaching Assistant: Hello August! How can I assist you today with your studies in computer science? If you have questions about Gradient Descent or any other topic, feel free to ask!\n",
      "Student: Can you explain me how ADAM optimization works?\n",
      "\n",
      "Teaching Assistant: Hello, August! I'd be happy to explain how Adam optimization works.\n",
      "\n",
      "Adam, short for Adaptive Moment Estimation, is an optimization algorithm that's used particularly in training neural networks. It combines ideas from two other extensions of stochastic gradient descent, specifically AdaGrad and RMSProp.\n",
      "\n",
      "1. **Adaptive Gradient Algorithm (AdaGrad)**: AdaGrad adapts the learning rates by scaling them inversely proportional to the square root of all past squared gradients. While this works well for sparse gradients, it may lead to an excessively rapid decrease in the learning rate for dense gradients (common in neural networks).\n",
      "\n",
      "2. **Root Mean Square Propagation (RMSProp)**: This algorithm modifies AdaGrad to alleviate its aggressive, monotonically decreasing learning rate. It does this by using a moving average of squared gradients to normalize the gradient itself. This adjustment helps to maintain a more suitable learning rate over time.\n",
      "\n",
      "Here’s how Adam essentially works step-by-step:\n",
      "- Adam computes adaptive learning rates for each parameter. \n",
      "- In addition to storing an exponentially decaying average of past squared gradients like RMSProp, Adam also keeps an exponentially decaying average of past gradients, similar to momentum.\n",
      "- It calculates the corrections for first and second moments (the means and the uncentered variances of the gradients, respectively) to account for their initial bias toward zero. \n",
      "- Combining these estimates, Adam adjusts the parameters to minimize the loss function of the neural network.\n",
      "\n",
      "Adam's advantages include relatively low memory requirements and being invariant to diagonal rescale of the gradients. Moreover, it is well suited for problems that are large in terms of data and/or parameters or when the gradients are sparse. Adam is also straightforward to implement and has been universally adopted in the deep learning community.\n",
      "\n",
      "I hope this helps you understand Adam optimizer! If you have more questions or need further clarification, feel free to ask.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"TAS v0 Conversations\"\n",
    "openai_client = wrap_openai(openai.Client())\n",
    "\n",
    "def output_dataset(inputs: dict) -> dict:\n",
    "    \"\"\"Extract entire chat history from dataset.\"\"\"\n",
    "    chat_hist = inputs[\"chat_history\"]\n",
    "    chat_string = \"\"\n",
    "    for message in chat_hist:\n",
    "        if message['type'] == \"ai\":\n",
    "            chat_string += f\"Teaching Assistant: {message['content']}\\n\\n\"\n",
    "        else:\n",
    "            chat_string += f\"Student: {message['content']}\\n\\n\"\n",
    "    return {\"chat_history\": chat_string}\n",
    "\n",
    "def is_answered(run: Run, example: Example) -> dict:\n",
    "\n",
    "    # Get outputs\n",
    "    conversation = run.outputs.get(\"chat_history\")\n",
    "\n",
    "    # Check if the student_answer is an empty string\n",
    "    if not conversation:\n",
    "        return {\"key\": \"is_answered\" , \"score\": 0}\n",
    "    else:\n",
    "        return {\"key\": \"is_answered\" , \"score\": 1}\n",
    "\n",
    "def conversation_length(run: Run, example: Example) -> dict:\n",
    "\n",
    "    # Get outputs\n",
    "    conversation = run.outputs.get(\"chat_history\")\n",
    "    print(conversation)\n",
    "    return {\"key\": \"conversation_lengthd\" , \"score\": len(conversation)}\n",
    "\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [is_answered, conversation_length]\n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    output_dataset,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-dbrx-qa-custom-eval-is-answered\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\"variant\": \"stuff website context into gpt-3.5-turbo\",},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-internal-example-47' at:\n",
      "https://smith.langchain.com/o/f6c953c1-69e2-5115-bc32-1c4edca6df0d/datasets/391da789-2074-43b9-a0eb-2970436e6929/compare?selectedSessions=cf78100b-890d-4ade-b44e-5d29c8c29844\n",
      "\n",
      "View all tests for Dataset TAS v1 Conversations at:\n",
      "https://smith.langchain.com/o/f6c953c1-69e2-5115-bc32-1c4edca6df0d/datasets/391da789-2074-43b9-a0eb-2970436e6929\n",
      "[------------------------------------------------->] 3/3"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.harmfulness</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c9ad869d-e574-4622-be07-d6b8b4d10e43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.013774</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.822373</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.155348</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.623366</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.091383</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.442988</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.794592</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.harmfulness error  execution_time  \\\n",
       "count                    3.0     0        3.000000   \n",
       "unique                   NaN     0             NaN   \n",
       "top                      NaN   NaN             NaN   \n",
       "freq                     NaN   NaN             NaN   \n",
       "mean                     0.0   NaN        2.013774   \n",
       "std                      0.0   NaN        0.822373   \n",
       "min                      0.0   NaN        1.155348   \n",
       "25%                      0.0   NaN        1.623366   \n",
       "50%                      0.0   NaN        2.091383   \n",
       "75%                      0.0   NaN        2.442988   \n",
       "max                      0.0   NaN        2.794592   \n",
       "\n",
       "                                      run_id  \n",
       "count                                      3  \n",
       "unique                                     3  \n",
       "top     c9ad869d-e574-4622-be07-d6b8b4d10e43  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define your runnable or chain below.\n",
    "prompt = prompts.ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\", \"You should extract the conversation history from the input.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "  ]\n",
    ")\n",
    "llm = chat_models.ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "chain = prompt | llm | output_parser.StrOutputParser()\n",
    "\n",
    "# Define the evaluators to apply\n",
    "eval_config = smith.RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # \"cot_qa\",\n",
    "        # smith.RunEvalConfig.LabeledCriteria(\"conciseness\"),\n",
    "        # smith.RunEvalConfig.LabeledCriteria(\"relevance\"),\n",
    "        # smith.RunEvalConfig.LabeledCriteria(\"coherence\"),\n",
    "        smith.RunEvalConfig.LabeledCriteria(\"harmfulness\"),\n",
    "        # smith.RunEvalConfig.LabeledCriteria(\"maliciousness\"),\n",
    "        # smith.RunEvalConfig.LabeledCriteria(\"helpfulness\"),\n",
    "        # smith.RunEvalConfig.LabeledCriteria(\"controversiality\"),\n",
    "        # smith.RunEvalConfig.LabeledCriteria(\"misogyny\"),\n",
    "        # smith.RunEvalConfig.LabeledCriteria(\"criminality\"),\n",
    "        # smith.RunEvalConfig.LabeledCriteria(\"insensitivity\")\n",
    "    ],\n",
    "    # custom_evaluators=[smith.RunEvalConfig.LabeledCriteria(\"Conversation Evaluator\")],\n",
    "    eval_llm=chat_models.ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\", temperature=0),\n",
    "    input_key=\"input\",\n",
    "    prediction_key=\"output\",\n",
    "    # reference_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "\n",
    "client = langsmith.Client()\n",
    "chain_results = client.run_on_dataset(\n",
    "    dataset_name=\"TAS v1 Conversations\",\n",
    "    llm_or_chain_factory=chain,\n",
    "    evaluation=eval_config,\n",
    "    project_name=\"test-internal-example-47\",\n",
    "    concurrency_level=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
