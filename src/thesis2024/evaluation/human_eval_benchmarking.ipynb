{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "import langsmith\n",
    "\n",
    "from human_eval.data import read_problems, write_jsonl\n",
    "\n",
    "from codechain.generation import HumanEvalChain, CompleteCodeChain\n",
    "from codechain.evaluation import HumanEvalEvaluator\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.smith import arun_on_dataset, RunEvalConfig\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://smith.langchain.com\", target=\"_blank\" rel=\"noopener\">LangSmith Client</a>"
      ],
      "text/plain": [
       "Client (API URL: https://api.smith.langchain.com)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Setup.\"\"\"\n",
    "\n",
    "\n",
    "# Set environment variables\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass(f\"Please provide your {var}\")\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "_set_if_undefined(\"TAVILY_API_KEY\")\n",
    "\n",
    "\n",
    "# Add tracing in LangSmith.\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "\n",
    "coding_agent_version = \"v1\"\n",
    "\n",
    "if coding_agent_version == \"v1\":\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"Coding Agetn v1: HumanEval Coding Benchmark\"\n",
    "elif coding_agent_version == \"v2\":\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"Coding Agetn v2: HumanEval Coding Benchmark\"\n",
    "elif coding_agent_version == \"v3\":\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"Coding Agetn v3: HumanEval Coding Benchmark\"\n",
    "\n",
    "\n",
    "# Dataset settings\n",
    "description = \"HumanEval dataset\"\n",
    "dataset_name= \"humaneval-small\" #\"humaneval-all\"\n",
    "max_problems = 3 #False\n",
    "repetitions_per_problem = 5\n",
    "\n",
    "\n",
    "# LLM settings\n",
    "provider = \"openai\"\n",
    "model_name = \"gpt-3.5-turbo-0125\" #\"gpt-4\"\n",
    "temperature = 0.2\n",
    "\n",
    "\n",
    "client = langsmith.Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "prompt = ChatPromptTemplate .from_messages ([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser ()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_one_completion(problem):\n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset creation.\"\"\"\n",
    "\n",
    "# Get the HumanEval dataset up to max_problems.\n",
    "problems = read_problems()\n",
    "if max_problems:\n",
    "    problems = {key: problems[key] for key in list(problems.keys())[:max_problems]}\n",
    "\n",
    "\n",
    "\n",
    "samples = [\n",
    "    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"]))\n",
    "    for task_id in problems\n",
    "    for _ in range(repetitions_per_problem)\n",
    "]\n",
    "write_jsonl(\"samples.jsonl\", samples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Dataset creation.\"\"\"\n",
    "\n",
    "# # Get the HumanEval dataset up to max_problems.\n",
    "# problems = read_problems()\n",
    "# if max_problems:\n",
    "#     problems = {key: problems[key] for key in list(problems.keys())[:max_problems]}\n",
    "\n",
    "\n",
    "# # If the dataset is new, update it to the LangSmith server.\n",
    "# if dataset_name not in set([dataset.name for dataset in client.list_datasets()]):\n",
    "#     dataset = client.create_dataset(dataset_name, description=description)\n",
    "#     for key, value in problems.items():\n",
    "#         client.create_example(\n",
    "#             inputs={\n",
    "#                 \"prompt\": value[\"prompt\"],\n",
    "#                 \"task_id\": key\n",
    "#                 },\n",
    "#             outputs={\n",
    "#                 \"canonical_solution\": value[\"canonical_solution\"],\n",
    "#                 },\n",
    "#             dataset_id=dataset.id\n",
    "#         )\n",
    "\n",
    "# \"\"\"Coding agent creation.\"\"\"\n",
    "\n",
    "# # Factory for the generation chain\n",
    "# def chain_factory():\n",
    "#     \"\"\"Create a code generation chain.\"\"\"\n",
    "\n",
    "#     llm_args = {\n",
    "#         \"model_name\": model_name,\n",
    "#         \"temperature\": temperature\n",
    "#     }\n",
    "#     if provider == \"openai\":\n",
    "#       llm = ChatOpenAI(**llm_args)\n",
    "\n",
    "#     return HumanEvalChain.from_llm(llm)\n",
    "\n",
    "# \"\"\"Evaluation.\"\"\"\n",
    "\n",
    "\n",
    "# # Evaluator configuration\n",
    "# evaluation = RunEvalConfig(\n",
    "#     custom_evaluators=[HumanEvalEvaluator()],\n",
    "#     input_key=\"task_id\"\n",
    "#     )\n",
    "\n",
    "# timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# # Run all generations and evaluations\n",
    "# for index in range(repetitions_per_problem):\n",
    "\n",
    "#     chain_results = await arun_on_dataset(\n",
    "#         client=client,\n",
    "#         dataset_name=dataset_name,\n",
    "#         project_name=f\"HumanEval {timestamp} — {index}\",\n",
    "#         concurrency_level=10,\n",
    "#         llm_or_chain_factory=chain_factory,\n",
    "#         evaluation=evaluation,\n",
    "#         tags=[\"HumanEval\"],\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "# import pandas as pd\n",
    "# from google.colab import data_table\n",
    "\n",
    "# project_names = [f\"HumanEval {timestamp} — {index}\" for index in range(0, repetitions_per_problem)]\n",
    "\n",
    "# # Retrieve a project from LangSmith\n",
    "# def get_project_as_df(project_name):\n",
    "#     print(f\"Downloading {project_name}\")\n",
    "#     runs = list(client.list_runs(project_name=project_name, execution_order=1))\n",
    "#     return pd.DataFrame(item.dict() for item in runs)\n",
    "\n",
    "# # Retrieve all projects and combine\n",
    "# dfs = [get_project_as_df(project_name) for project_name in project_names]\n",
    "# df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# # Display the results\n",
    "# data_table.enable_dataframe_formatter()\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
