{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Debugging and Running the PMAS Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the PMAS Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis2024.PMAS import PMAS\n",
    "from thesis2024.utils import init_llm_langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short and consise explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFrom the conversation between August and the Teaching Assistant, it is possible to deduce certain learning preferences that August may have, which are indicated by specific parts of the dialogue.\n",
      "\n",
      "1. **Preference for Concise Information**: August expressed a preference for more concise explanations when he mentioned that the initial detailed explanation was too much text for him. This suggests that he might benefit from bullet points, summaries, or simplified explanations rather than lengthy, detailed descriptions.\n",
      "   - **Indicating snippet**: \"Wow that was a long explanation, way to much text for me.. can you boil it down a little?\"\n",
      "\n",
      "2. **Visual Learning**: While not explicitly stated, August's request for further elaboration on formulas after receiving a simplified explanation suggests that he might find visual representations like formulas, graphs, or charts helpful in understanding concepts better.\n",
      "   - **Indicating snippet**: \"Ahh thats much better. Can you elaborate further on the relevant formulas, in case I have to be able to describe them at the exam\"\n",
      "\n",
      "3. **Active Engagement and Examples**: August's engagement with the Teaching Assistant in asking for a trial exam question and attempting to answer it suggests that he benefits from active learning strategies. This includes practical exercises, problem-solving, and direct application of concepts.\n",
      "   - **Indicating snippet**: \"Can you give me a trial exam question that I have to answer?\"\n",
      "\n",
      "Based on these observations, the following learning techniques should be considered for August:\n",
      "- **Use of succinct and clear summaries** to introduce and wrap up complex topics.\n",
      "- **Incorporation of visual aids** such as diagrams, formulas on the board, and step-by-step breakdowns of mathematical concepts.\n",
      "- **Active learning opportunities** through quizzes, practical examples, and problem-solving sessions to enhance engagement and understanding.\n",
      "\n",
      "These strategies align with the snippets from the conversation where August shows a preference for less text-heavy explanations, seeks clarification on formulas, and engages actively with example problems. These interactions clearly show a student who benefits from concise, visually supported, and interactive learning environments.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer:\n",
      "1. Yes, the conversation did indicate which learning preferences the student, August, may have.\n",
      "2. The learning preferences identified from this conversation are:\n",
      "   - Preference for concise information: August prefers succinct explanations rather than lengthy details.\n",
      "   - Visual learning: August seems to benefit from visual representations like formulas.\n",
      "   - Active engagement and examples: August engages well with practical exercises and direct application of concepts.\n",
      "3. The parts of the conversation that indicated these learning preferences include:\n",
      "   - **Preference for concise information**: \"Wow that was a long explanation, way too much text for me.. can you boil it down a little?\"\n",
      "   - **Visual learning**: \"Ahh thats much better. Can you elaborate further on the relevant formulas, in case I have to be able to describe them at the exam\"\n",
      "   - **Active engagement and examples**: \"Can you give me a trial exam question that I have to answer?\" and the subsequent attempt to answer it.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " 1. Yes, the conversation did indicate which learning preferences the student, August, may have.\n",
      "2. The learning preferences identified from this conversation are:\n",
      "   - Preference for concise information: August prefers succinct explanations rather than lengthy details.\n",
      "   - Visual learning: August seems to benefit from visual representations like formulas.\n",
      "   - Active engagement and examples: August engages well with practical exercises and direct application of concepts.\n",
      "3. The parts of the conversation that indicated these learning preferences include:\n",
      "   - **Preference for concise information**: \"Wow that was a long explanation, way too much text for me.. can you boil it down a little?\"\n",
      "   - **Visual learning**: \"Ahh thats much better. Can you elaborate further on the relevant formulas, in case I have to be able to describe them at the exam\"\n",
      "   - **Active engagement and examples**: \"Can you give me a trial exam question that I have to answer?\" and the subsequent attempt to answer it.\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello, how are you today?\\nTeaching Assistant: Hello, August! I'm here to assist you. How can I help you with your Intro to Machine Learning course today?\\nStudent: I am trying to learn about methods like linear regression\\nTeaching Assistant: Linear regression is a statistical method used in machine learning to predict a continuous outcome variable (y) based on one or more predictor variables (x). The goal is to find a linear relationship between these variables, which can be described by the equation:\\n$$ y = \\\\beta_0 + \\\\beta_1x_1 + \\\\beta_2x_2 + ... + \\\\beta_nx_n + \\\\epsilon $$\\nwhere:\\n- \\\\( \\\\beta_0, \\\\beta_1, ..., \\\\beta_n \\\\) are coefficients that the model learns to optimize.\\n- \\\\( \\\\epsilon \\\\) is the error term, accounting for the randomness in the data.\\n\\n### Simple Linear Regression Example\\nConsider a scenario where you have data points that represent the relationship between the amount of hours studied (x) and the scores obtained in an exam (y). In simple linear regression, you would try to fit a line that best predicts the scores based on the hours studied. This line is represented by:\\n$$ y = \\\\beta_0 + \\\\beta_1x + \\\\epsilon $$\\n\\n### Feature Transformation in Linear Regression\\nIn more complex scenarios, such as predicting a value based on multiple inputs, you might need to transform features to capture non-linear relationships. For instance, if you're considering both the hours studied and the type of subject being studied, you might include polynomial terms (like \\\\(x^2\\\\)) or interactions between features (like \\\\(x_1 \\\\times x_2\\\\)) to improve your model. This is represented as:\\n$$ y = \\\\beta_0 + \\\\beta_1x_1 + \\\\beta_2x_2 + \\\\beta_3x_1^2 + \\\\beta_4x_2^2 + \\\\beta_5x_1x_2 + \\\\epsilon $$\\n\\n### Applications\\nLinear regression can be applied in various fields:\\n- In finance, to predict stock prices based on historical performance and other economic indicators.\\n- In health care, to predict patient outcomes based on treatment protocols and patient characteristics.\\n- In marketing, to predict sales based on advertising spend across different media channels.\\n\\n### Practical Example Code\\nHere is a simple Python example using the `scikit-learn` library to perform linear regression:\\n```python\\nfrom sklearn.linear_model import LinearRegression\\nimport numpy as np\\n\\n# Example data\\nX = np.array([[1], [2], [3], [4], [5]])  # Hours studied\\ny = np.array([2, 4, 5, 4, 5])            # Exam scores\\n\\n# Create a linear regression model\\nmodel = LinearRegression()\\nmodel.fit(X, y)\\n\\n# Predict using the model\\npredicted_scores = model.predict(X)\\n\\nprint(\\\"Coefficients:\\\", model.coef_)\\nprint(\\\"Intercept:\\\", model.intercept_)\\nprint(\\\"Predicted Scores:\\\", predicted_scores)\\n```\\n\\nThis code snippet creates a model to predict exam scores based on the hours studied, fits the model to the data, and then uses it to make predictions.\\nStudent: Wow that was a long explanation, way to much text for me.. can you boil it down a little?\\nTeaching Assistant: Linear regression is a method used to predict a continuous variable based on one or more other variables. It finds the best line that fits the data, using an equation of the form:\\n$$ y = \\\\beta_0 + \\\\beta_1x_1 + \\\\beta_2x_2 + ... + \\\\beta_nx_n + \\\\epsilon $$\\nHere, $\\\\beta_0, \\\\beta_1, ..., \\\\beta_n$ are coefficients the model optimizes, and $\\\\epsilon$ represents error or noise in the data. This technique is useful in various fields like finance, healthcare, and marketing to make predictions based on historical data.\\nStudent: Ahh thats much better. Can you elaborate further on the relevant formulas, in case I have to be able to describe them at the exam\\nTeaching Assistant: In linear regression, the relationship between the dependent variable \\\\( y \\\\) and one or more independent variables \\\\( x \\\\) is modeled. The formulas involved are crucial for understanding how predictions are made.\\n\\n### 1. Simple Linear Regression Formula:\\nIn simple linear regression, where there is only one predictor variable, the formula is:\\n$$ y = \\\\beta_0 + \\\\beta_1x + \\\\epsilon $$\\nHere:\\n- \\\\( \\\\beta_0 \\\\) is the y-intercept of the regression line.\\n- \\\\( \\\\beta_1 \\\\) is the slope of the regression line, indicating the effect of the predictor variable \\\\( x \\\\) on the dependent variable \\\\( y \\\\).\\n- \\\\( \\\\epsilon \\\\) represents the error term, which accounts for the variability in \\\\( y \\\\) that cannot be explained by \\\\( x \\\\).\\n\\n### 2. Multiple Linear Regression Formula:\\nWhen there are multiple predictor variables, the formula extends to:\\n$$ y = \\\\beta_0 + \\\\beta_1x_1 + \\\\beta_2x_2 + ... + \\\\beta_nx_n + \\\\epsilon $$\\nHere:\\n- \\\\( \\\\beta_0, \\\\beta_1, ..., \\\\beta_n \\\\) are the coefficients for each predictor variable \\\\( x_1, x_2, ..., x_n \\\\).\\n- \\\\( \\\\epsilon \\\\) is again the error term.\\n\\n### 3. Estimation of Coefficients:\\nThe coefficients \\\\( \\\\beta_0, \\\\beta_1, ..., \\\\beta_n \\\\) are estimated using the method of least squares, which minimizes the sum of the squared differences between the observed values and the values predicted by the model:\\n$$ \\\\text{Minimize} \\\\sum_{i=1}^n (y_i - (\\\\beta_0 + \\\\beta_1x_{1i} + ... + \\\\beta_nx_{ni}))^2 $$\\n\\n### 4. Assumptions:\\nFor the linear regression model to provide valid predictions, several assumptions must be met:\\n- Linearity: The relationship between the predictors and the outcome is linear.\\n- Homoscedasticity: The variance of residual is the same for any value of \\\\( x \\\\).\\n- Independence: Observations are independent of each other.\\n- Normality: For any fixed value of \\\\( x \\\\), \\\\( y \\\\) is normally distributed.\\n\\nUnderstanding these formulas and assumptions is crucial for effectively applying linear regression analysis and interpreting its results. This knowledge will be particularly useful for your exam in demonstrating a thorough understanding of how linear regression works.\\nStudent: Ok! So the least squares is the main method for estimating coefficients, but are there others?\\nTeaching Assistant: Yes, August, besides the least squares method, there are several other methods for estimating coefficients in regression models:\\n\\n1. **Regularization Techniques**: These are modifications of the least squares method that add a penalty term to the loss function you are trying to minimize. Examples include:\\n   - **Ridge Regression (L2 Regularization)**: Adds a penalty equal to the square of the magnitude of coefficients.\\n   - **Lasso Regression (L1 Regularization)**: Adds a penalty equal to the absolute value of the magnitude of coefficients.\\n   - **Elastic Net**: Combines penalties of Lasso and Ridge.\\n   Regularization techniques help prevent overfitting by reducing model complexity.\\n\\n2. **Nearest-Neighbor Methods**: Unlike the global approach of least squares, nearest-neighbor methods estimate the response at a point based on the responses of its nearest neighbors in the training set. This can be more flexible than least squares and can adapt to more complex patterns in the data.\\n\\n3. **Cross-Validation**: While not a method for estimating coefficients directly, cross-validation is a technique used to evaluate the effectiveness of your regression model, including the choice of coefficient estimation method. It involves dividing the data into subsets, using some subsets to train the model and the remaining subset to test the model. This helps in selecting the model that generalizes best to unseen data.\\n\\nEach of these methods has its advantages and is chosen based on the specific requirements and characteristics of the data you are working with.\\nStudent: What is the formula for L2?\\nTeaching Assistant: L2 regularization, also known as Ridge Regression, is a technique used to prevent overfitting in machine learning models, particularly in the context of regression. The formula for L2 regularization involves adding a penalty term to the usual least squares loss function, which is proportional to the square of the magnitude of the coefficients. This penalty term is controlled by a parameter \\\\( \\\\lambda \\\\), which is a non-negative hyperparameter.\\n\\nThe modified loss function in L2 regularization is given by:\\n$$\\n\\\\text{Loss} = \\\\sum_{i=1}^n (y_i - \\\\hat{y}_i)^2 + \\\\lambda \\\\sum_{j=1}^p \\\\beta_j^2\\n$$\\nwhere:\\n- \\\\( y_i \\\\) are the actual observed responses,\\n- \\\\( \\\\hat{y}_i \\\\) are the predicted responses from the model,\\n- \\\\( \\\\beta_j \\\\) are the model coefficients,\\n- \\\\( n \\\\) is the number of data points,\\n- \\\\( p \\\\) is the number of features (or predictors),\\n- \\\\( \\\\lambda \\\\) is the regularization parameter that controls the trade-off between fitting the model well on the training data and keeping the model coefficients small to avoid overfitting.\\n\\nThe inclusion of the \\\\( \\\\lambda \\\\sum_{j=1}^p \\\\beta_j^2 \\\\) term encourages the coefficients to shrink towards zero, but not exactly zero, which helps in reducing model complexity and variance without substantially increasing the bias.\\nStudent: Can you give me a trial exam question that I have to answer?\\nTeaching Assistant: Here's a trial exam question for you, August:\\n\\n**Question:**\\nYou are provided with a dataset containing housing prices in a city, where the main features include the size of the house (in square feet), the number of bedrooms, and the age of the house (in years). You are tasked with developing a linear regression model to predict the price of the houses.\\n\\n1. **Model Formulation:**\\n   - Write down the equation for a multiple linear regression model that could be used for predicting house prices based on the given features.\\n\\n2. **Coefficient Estimation:**\\n   - Explain how you would use the least squares method to estimate the coefficients of your model.\\n\\n3. **Regularization:**\\n   - Suppose you notice that your model is overfitting the training data. Describe how you could modify your model to include Ridge Regression (L2 regularization) and explain how this helps reduce overfitting.\\n\\n4. **Model Evaluation:**\\n   - Discuss how you would evaluate the performance of your model. Mention at least two metrics that you would use and explain why they are appropriate for this task.\\n\\nThis question covers several key aspects of linear regression and tests your ability to apply these concepts to a practical problem.\\nStudent: I thing the model looks something like this:\\nPrice=coeff_A * housesize + coeff_B * numberofbedrooms + coeff_C * age + error\\n\\nI dont know, can you explain the rest?\\nTeaching Assistant: August, the equation you mentioned:\\n$$ \\\\text{Price} = \\\\text{coeff}_A \\\\times \\\\text{housesize} + \\\\text{coeff}_B \\\\times \\\\text{numberofbedrooms} + \\\\text{coeff}_C \\\\times \\\\text{age} + \\\\text{error} $$\\nrepresents a multiple linear regression model. Here's what each component represents:\\n\\n1. **Dependent Variable (Price):** This is the variable you are trying to predict. In this case, it's the price of the house.\\n\\n2. **Independent Variables (housesize, numberofbedrooms, age):** These are the features or predictors that you believe influence the dependent variable. Each feature has an associated coefficient which quantifies the impact of that feature on the price:\\n   - **coeff_A:** This coefficient multiplies the size of the house (housesize). It represents the change in the price of the house for each unit increase in the size of the house, holding all other factors constant.\\n   - **coeff_B:** This coefficient is associated with the number of bedrooms (numberofbedrooms). It quantifies the additional price expected due to an increase in the number of bedrooms, assuming other factors remain the same.\\n   - **coeff_C:** This coefficient is linked to the age of the house (age). It shows how the price is expected to change as the house ages.\\n\\n3. **Error Term (error):** This term accounts for the variability in house price that cannot be explained by the house size, number of bedrooms, and age alone. It includes factors not present in the model, measurement errors, and other forms of randomness.\\n\\nWhen you use this model for prediction, you'll estimate the coefficients (coeff_A, coeff_B, coeff_C) from historical data using a method like least squares, which minimizes the sum of squared differences between observed and predicted prices. If you find that the model overfits the training data, you might consider regularization techniques like Ridge Regression to reduce overfitting by penalizing large coefficients.\\n\\nThis model can help you make informed predictions about house prices based on the specified features, and understanding each component will allow you to interpret the results effectively and make adjustments as needed.\\nStudent: Ahh that makes sense\\nTeaching Assistant: ### Coefficient Estimation:\\nTo estimate the coefficients (\\\\(\\\\text{coeff}_A\\\\), \\\\(\\\\text{coeff}_B\\\\), \\\\(\\\\text{coeff}_C\\\\)) using the least squares method, you would follow these steps:\\n1. **Set Up the Equation Matrix:** Organize your data into a matrix format where each row represents a data point and each column represents a predictor variable (housesize, numberofbedrooms, age). Add an additional column of ones to represent the intercept (\\\\(\\\\text{coeff}_0\\\\)).\\n2. **Apply the Least Squares Formula:** Use the formula \\\\(\\\\beta = (X^TX)^{-1}X^Ty\\\\) to calculate the coefficients. Here, \\\\(X\\\\) is your matrix of predictors, \\\\(y\\\\) is the vector of observed prices, and \\\\(\\\\beta\\\\) is the vector of coefficients including the intercept.\\n3. **Interpret the Results:** The output will give you the estimated coefficients that minimize the sum of squared residuals between the observed and predicted prices.\\n\\n### Regularization (Ridge Regression):\\nIf you notice that your model is overfitting:\\n1. **Modify the Loss Function:** Incorporate a penalty term into the least squares loss function. The Ridge Regression formula is:\\n   \\\\[\\n   \\\\text{Loss} = \\\\sum_{i=1}^n (y_i - \\\\hat{y}_i)^2 + \\\\lambda \\\\sum_{j=0}^p \\\\beta_j^2\\n   \\\\]\\n   Here, \\\\(\\\\lambda\\\\) is the regularization parameter that you choose to control the impact of the penalty term.\\n2. **Fit the Model:** Use this modified loss function to fit your model. The penalty term will shrink the coefficients towards zero, reducing their magnitude and thus helping to prevent overfitting by not allowing the model to fit excessively to the noise in the training data.\\n\\n### Model Evaluation:\\nTo evaluate the performance of your model, consider using these metrics:\\n1. **Root Mean Squared Error (RMSE):** This metric measures the average magnitude of the errors between the predicted and actual values, giving you an idea of how close the predictions are to the actual prices.\\n   \\\\[\\n   \\\\text{RMSE} = \\\\sqrt{\\\\frac{\\\\sum_{i=1}^n (\\\\hat{y}_i - y_i)^2}{n}}\\n   \\\\]\\n2. **R-squared (\\\\(R^2\\\\)):** This metric indicates the proportion of variance in the dependent variable that is predictable from the independent variables. It provides a sense of how well unseen samples are likely to be predicted by the model.\\n   \\\\[\\n   R^2 = 1 - \\\\frac{\\\\sum_{i=1}^n (y_i - \\\\hat{y}_i)^2}{\\\\sum_{i=1}^n (y_i - \\\\bar{y})^2}\\n   \\\\]\\n   where \\\\(\\\\bar{y}\\\\) is the mean of the observed data.\\n\\nThese explanations should help you understand and complete your trial exam question effectively, demonstrating your grasp of linear regression concepts and their practical application.\\nStudent: I did not ask you to explain it again, but than you Iguess\\nTeaching Assistant: I'm glad that the explanation made sense to you, August! If you have any more questions or need further clarification on any aspect of linear regression or any other topic in your course, feel free to ask. Good luck with your studies!\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning through examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe conversation indicates that the student, August, has specific learning preferences, particularly in understanding complex topics through concrete examples and step-by-step explanations.\n",
      "\n",
      "1. **Learning through Examples**: August repeatedly asks for practical examples after receiving explanations about the concepts. This suggests a preference for learning through concrete examples that illustrate theoretical concepts.\n",
      "2. **Step-by-Step Learning**: The student seems to benefit from detailed, step-by-step breakdowns of processes, as shown by the Teaching Assistant's detailed explanation of bagging and its steps, which appeared to clarify the concept for August.\n",
      "\n",
      "**Indicative Parts of the Conversation**:\n",
      "- **Learning through Examples**: \"Can you give an example of a case in which bagging can be used?\" This question shows August's need to understand concepts through practical applications.\n",
      "- **Step-by-Step Learning**: The detailed explanation of bagging, including the steps involved (data sampling, model training, aggregation of predictions), seems to help August grasp the concept, as indicated by his response, \"Ahh I think that makes sense!\"\n",
      "\n",
      "Final Answer:\n",
      "1. Yes, the conversation indicated which learning preferences the student may have.\n",
      "2. The learning preferences identified from this conversation are learning through examples and step-by-step explanations.\n",
      "3. Indications of these preferences include:\n",
      "   - **Learning through Examples**: \"Can you give an example of a case in which bagging can be used?\"\n",
      "   - **Step-by-Step Learning**: The detailed breakdown of the bagging process and its subsequent clarification for the student.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " 1. Yes, the conversation indicated which learning preferences the student may have.\n",
      "2. The learning preferences identified from this conversation are learning through examples and step-by-step explanations.\n",
      "3. Indications of these preferences include:\n",
      "   - **Learning through Examples**: \"Can you give an example of a case in which bagging can be used?\"\n",
      "   - **Step-by-Step Learning**: The detailed breakdown of the bagging process and its subsequent clarification for the student.\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello! I am August. I would like to learn about ensemble methods\\nTeaching Assistant: Hello August! I'm glad you're interested in ensemble methods in machine learning. Let's dive into what they are and why they are useful.\\n\\nEnsemble methods involve combining the outputs of multiple models to create a single model with superior predictive performance. This approach is analogous to seeking opinions from multiple experts to form a more reliable judgment.\\n\\nHere’s a simple breakdown:\\n1. **Multiple Models**: You train multiple models on your data. These could be any type of models - decision trees, neural networks, etc.\\n2. **Combine Outputs**: After training, instead of using just one model’s output, you aggregate the outputs from all the models. This aggregation might be an average, a majority vote, or another method depending on whether you are dealing with regression or classification tasks.\\n\\nThe main advantages of using ensemble methods include:\\n- **Reduced Variance**: By averaging multiple models, the ensemble model is less likely to fit too closely to the training data (overfit), making it more robust and generalizable.\\n- **Improved Accuracy**: Often, ensemble methods perform better than any single model in the ensemble, especially if the individual models make different types of errors.\\n\\nTwo popular techniques in ensemble methods are:\\n- **Bagging**: Short for Bootstrap Aggregating, it involves training multiple models on different subsets of the dataset (sampled with replacement), and then averaging their predictions. An example is the Random Forest algorithm, which applies bagging to decision trees.\\n- **Boosting**: This method trains models sequentially, each trying to correct the errors of the previous ones. The outputs are then combined through a weighted sum approach. AdaBoost is a well-known boosting algorithm.\\n\\nEnsemble methods are a powerful tool in machine learning, especially when you need robustness and accuracy from your predictive models. If you need more specific examples or deeper insights into any particular method, feel free to ask!\\nStudent: Can you explain it bagging further?\\nTeaching Assistant: Bagging, short for Bootstrap Aggregating, is an ensemble method used in machine learning to improve the stability and accuracy of machine learning algorithms. It involves the following key steps:\\n\\n1. **Multiple Subsets**: Bagging begins by creating multiple subsets of the original dataset. Each subset is created through a process called bootstrapping, which means sampling with replacement. This means that some observations may appear more than once in a single subset while others may not appear at all.\\n\\n2. **Training Multiple Models**: Each of these subsets is used to train a separate model. Typically, the same type of model is used for each subset but trained on different data. For instance, if you are using decision trees, each tree will be trained on a different subset.\\n\\n3. **Aggregating Outputs**: Once all models are trained, their outputs (predictions) are combined into a single output. The combination method often depends on the type of task:\\n   - For regression problems, the average of the predictions is typically used.\\n   - For classification, the majority vote (or sometimes a weighted vote) is used.\\n\\nThe main advantages of bagging include:\\n- **Reduction in Variance**: By averaging multiple estimates, bagging reduces the chances of overfitting to the noise in the training data, which can be a significant problem in complex models.\\n- **Improvement in Accuracy**: Bagging can lead to improvements in accuracy, especially for models that have high variance.\\n\\nA classic example of bagging in practice is the Random Forest algorithm, which applies the bagging technique to decision trees. In Random Forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. Furthermore, when splitting a node during the construction of the tree, the best split is found from a random subset of the features, adding an extra layer of randomness that helps decrease the correlation between trees in the forest, thus increasing the ensemble's overall performance.\\n\\nBagging is particularly effective when the base models are high variance and low bias. Since each model is trained independently, bagging can also be easily parallelized, making it efficient for large-scale applications.\\nStudent: Can you give an example of a case in which bagging can be used?\\nTeaching Assistant: A practical example of where bagging can be used is in credit risk assessment. In this scenario, financial institutions need to predict whether a loan applicant is likely to default on their loan repayments. The challenge is that the dataset might contain complex patterns and noise, leading to a high variance if a single predictive model is used.\\n\\nHere's how bagging can be applied:\\n1. **Data Sampling**: Multiple subsets of the original loan applicant dataset are created using bootstrapping, where each subset may include some applicants more than once and others not at all.\\n2. **Model Training**: A decision tree model is trained on each subset. Decision trees are chosen because they are sensitive to variations in the dataset and can capture complex patterns in the data.\\n3. **Aggregation of Predictions**: After training, the predictions from each decision tree are aggregated to make a final prediction for each applicant. For a binary outcome (default or no default), the aggregation could be a majority vote among the predictions from all trees.\\n\\nBy using bagging, the bank can reduce the variance of the predictions, leading to a more reliable assessment of credit risk. This method helps in making more accurate decisions, thereby reducing the risk of financial loss due to loan defaults.\\n\\nThis example illustrates how bagging helps in managing high-variance models and improves prediction reliability, making it a valuable technique in critical financial decision-making processes.\\nStudent: Ahh I think that makes sense! Can you give another example?\\nTeaching Assistant: Another example of where ensemble methods can be effectively used is in weather forecasting. In this scenario, the goal is to predict weather conditions such as temperature, precipitation, and wind speed. Weather patterns are incredibly complex and influenced by a myriad of factors, making single model predictions often unreliable.\\n\\nHere's how ensemble methods are applied in weather forecasting:\\n1. **Multiple Models**: Different meteorological models are used to predict weather conditions. These models may vary in terms of the factors they consider (like atmospheric conditions, ocean temperatures, etc.) and their computational approaches.\\n2. **Model Training**: Each model is trained using historical weather data. This training allows each model to capture different aspects or patterns of weather behavior.\\n3. **Aggregation of Predictions**: The predictions from each model are then combined to form a single forecast. This aggregation might be done by taking the average of the predictions or using more sophisticated methods like weighted averages where more accurate models are given higher weights.\\n\\nThe use of ensemble methods in weather forecasting helps in improving the accuracy and reliability of the predictions. By combining multiple models, forecasters can mitigate the risk of relying on a single model that might have specific biases or limitations. This method is crucial for making better-informed decisions in disaster management, agriculture planning, and daily life activities dependent on weather conditions.\\n\\nThis example demonstrates the versatility of ensemble methods and their applicability in critical real-world scenarios where decision-making is heavily reliant on predictive accuracy.\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mBased on the conversation between August and the Teaching Assistant, it seems August is interested in understanding complex machine learning concepts, specifically focusing on ensemble methods and their applications. He has asked for further explanations and examples to deepen his understanding.\n",
      "\n",
      "The conversation suggests that August may benefit from learning through examples and practical applications of concepts. His repeated requests for examples indicate a preference for contextual learning where abstract concepts are tied to real-world scenarios. This method seems to help him grasp the theoretical aspects more effectively.\n",
      "\n",
      "Final Answer:\n",
      "{\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Learning through examples\", \"Contextual learning\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you explain it bagging further?\",\n",
      "    \"Student: Can you give an example of a case in which bagging can be used?\",\n",
      "    \"Student: Ahh I think that makes sense! Can you give another example?\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " {\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Learning through examples\", \"Contextual learning\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you explain it bagging further?\",\n",
      "    \"Student: Can you give an example of a case in which bagging can be used?\",\n",
      "    \"Student: Ahh I think that makes sense! Can you give another example?\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello! I am August. I would like to learn about ensemble methods\\nTeaching Assistant: Hello August! I'm glad you're interested in ensemble methods in machine learning. Let's dive into what they are and why they are useful.\\n\\nEnsemble methods involve combining the outputs of multiple models to create a single model with superior predictive performance. This approach is analogous to seeking opinions from multiple experts to form a more reliable judgment.\\n\\nHere’s a simple breakdown:\\n1. **Multiple Models**: You train multiple models on your data. These could be any type of models - decision trees, neural networks, etc.\\n2. **Combine Outputs**: After training, instead of using just one model’s output, you aggregate the outputs from all the models. This aggregation might be an average, a majority vote, or another method depending on whether you are dealing with regression or classification tasks.\\n\\nThe main advantages of using ensemble methods include:\\n- **Reduced Variance**: By averaging multiple models, the ensemble model is less likely to fit too closely to the training data (overfit), making it more robust and generalizable.\\n- **Improved Accuracy**: Often, ensemble methods perform better than any single model in the ensemble, especially if the individual models make different types of errors.\\n\\nTwo popular techniques in ensemble methods are:\\n- **Bagging**: Short for Bootstrap Aggregating, it involves training multiple models on different subsets of the dataset (sampled with replacement), and then averaging their predictions. An example is the Random Forest algorithm, which applies bagging to decision trees.\\n- **Boosting**: This method trains models sequentially, each trying to correct the errors of the previous ones. The outputs are then combined through a weighted sum approach. AdaBoost is a well-known boosting algorithm.\\n\\nEnsemble methods are a powerful tool in machine learning, especially when you need robustness and accuracy from your predictive models. If you need more specific examples or deeper insights into any particular method, feel free to ask!\\nStudent: Can you explain it bagging further?\\nTeaching Assistant: Bagging, short for Bootstrap Aggregating, is an ensemble method used in machine learning to improve the stability and accuracy of machine learning algorithms. It involves the following key steps:\\n\\n1. **Multiple Subsets**: Bagging begins by creating multiple subsets of the original dataset. Each subset is created through a process called bootstrapping, which means sampling with replacement. This means that some observations may appear more than once in a single subset while others may not appear at all.\\n\\n2. **Training Multiple Models**: Each of these subsets is used to train a separate model. Typically, the same type of model is used for each subset but trained on different data. For instance, if you are using decision trees, each tree will be trained on a different subset.\\n\\n3. **Aggregating Outputs**: Once all models are trained, their outputs (predictions) are combined into a single output. The combination method often depends on the type of task:\\n   - For regression problems, the average of the predictions is typically used.\\n   - For classification, the majority vote (or sometimes a weighted vote) is used.\\n\\nThe main advantages of bagging include:\\n- **Reduction in Variance**: By averaging multiple estimates, bagging reduces the chances of overfitting to the noise in the training data, which can be a significant problem in complex models.\\n- **Improvement in Accuracy**: Bagging can lead to improvements in accuracy, especially for models that have high variance.\\n\\nA classic example of bagging in practice is the Random Forest algorithm, which applies the bagging technique to decision trees. In Random Forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. Furthermore, when splitting a node during the construction of the tree, the best split is found from a random subset of the features, adding an extra layer of randomness that helps decrease the correlation between trees in the forest, thus increasing the ensemble's overall performance.\\n\\nBagging is particularly effective when the base models are high variance and low bias. Since each model is trained independently, bagging can also be easily parallelized, making it efficient for large-scale applications.\\nStudent: Can you give an example of a case in which bagging can be used?\\nTeaching Assistant: A practical example of where bagging can be used is in credit risk assessment. In this scenario, financial institutions need to predict whether a loan applicant is likely to default on their loan repayments. The challenge is that the dataset might contain complex patterns and noise, leading to a high variance if a single predictive model is used.\\n\\nHere's how bagging can be applied:\\n1. **Data Sampling**: Multiple subsets of the original loan applicant dataset are created using bootstrapping, where each subset may include some applicants more than once and others not at all.\\n2. **Model Training**: A decision tree model is trained on each subset. Decision trees are chosen because they are sensitive to variations in the dataset and can capture complex patterns in the data.\\n3. **Aggregation of Predictions**: After training, the predictions from each decision tree are aggregated to make a final prediction for each applicant. For a binary outcome (default or no default), the aggregation could be a majority vote among the predictions from all trees.\\n\\nBy using bagging, the bank can reduce the variance of the predictions, leading to a more reliable assessment of credit risk. This method helps in making more accurate decisions, thereby reducing the risk of financial loss due to loan defaults.\\n\\nThis example illustrates how bagging helps in managing high-variance models and improves prediction reliability, making it a valuable technique in critical financial decision-making processes.\\nStudent: Ahh I think that makes sense! Can you give another example?\\nTeaching Assistant: Another example of where ensemble methods can be effectively used is in weather forecasting. In this scenario, the goal is to predict weather conditions such as temperature, precipitation, and wind speed. Weather patterns are incredibly complex and influenced by a myriad of factors, making single model predictions often unreliable.\\n\\nHere's how ensemble methods are applied in weather forecasting:\\n1. **Multiple Models**: Different meteorological models are used to predict weather conditions. These models may vary in terms of the factors they consider (like atmospheric conditions, ocean temperatures, etc.) and their computational approaches.\\n2. **Model Training**: Each model is trained using historical weather data. This training allows each model to capture different aspects or patterns of weather behavior.\\n3. **Aggregation of Predictions**: The predictions from each model are then combined to form a single forecast. This aggregation might be done by taking the average of the predictions or using more sophisticated methods like weighted averages where more accurate models are given higher weights.\\n\\nThe use of ensemble methods in weather forecasting helps in improving the accuracy and reliability of the predictions. By combining multiple models, forecasters can mitigate the risk of relying on a single model that might have specific biases or limitations. This method is crucial for making better-informed decisions in disaster management, agriculture planning, and daily life activities dependent on weather conditions.\\n\\nThis example demonstrates the versatility of ensemble methods and their applicability in critical real-world scenarios where decision-making is heavily reliant on predictive accuracy.\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference for methafors, implicitely stated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFrom the conversation, it appears that August has specific learning preferences that can be identified from the interaction with the Teaching Assistant.\n",
      "\n",
      "Thought: To determine August's learning preferences, I need to analyze the conversation and identify moments where August's learning style or preferences are indicated through their questions and responses.\n",
      "\n",
      "Action: Review the conversation snippets provided to identify learning preferences.\n",
      "\n",
      "Action Input: Analyzing the conversation for indications of learning preferences.\n",
      "\u001b[0mReview the conversation snippets provided to identify learning preferences. is not a valid tool, try one of [Web Search, Retrieval Tool].\u001b[32;1m\u001b[1;3mSince I cannot directly review the conversation snippets as an action, I will analyze the provided conversation in my processing to identify learning preferences.\n",
      "\n",
      "Thought: In the conversation, August initially asks about Bayesian probabilities and then requests an explanation of the formulas related to Bayesian probabilities, indicating a preference for understanding theoretical concepts through formulas. However, when the formulas are provided, August asks for another explanation method, suggesting a preference for more intuitive or practical explanations.\n",
      "\n",
      "Action: Analyze the specific parts of the conversation where August asks for explanations in different forms to understand their learning preferences better.\n",
      "\n",
      "Action Input: Reviewing the parts where August reacts to explanations and asks for different forms of explanation.\n",
      "\u001b[0mAnalyze the specific parts of the conversation where August asks for explanations in different forms to understand their learning preferences better. is not a valid tool, try one of [Web Search, Retrieval Tool].\u001b[32;1m\u001b[1;3mSince I cannot use tools to directly analyze the conversation, I will continue processing the information internally to draw conclusions about August's learning preferences based on the conversation snippets.\n",
      "\n",
      "Thought: August shows a preference for analogies and practical explanations over purely theoretical or formulaic descriptions. This is evident when August requests the formulas to be explained in \"another way\" after the Teaching Assistant provides the mathematical expression of Bayes' Theorem. The Teaching Assistant then uses a detective analogy to explain Bayesian probabilities, which August finds helpful.\n",
      "\n",
      "Thought: The conversation snippets where August's learning preferences are indicated include:\n",
      "1. \"What are the formulas related to bayesian probs?\" - This shows an initial attempt to understand through theoretical and formulaic means.\n",
      "2. \"Actually, I think I need these formulas explained in another way to understand bayesian probabilities\" - This indicates a preference for different explanatory methods.\n",
      "3. \"Ahh it makes sense now, thank you!\" - This response after receiving the analogy suggests that analogical and practical explanations are more effective for August.\n",
      "\n",
      "Thought: I now know the final answer.\n",
      "\n",
      "Final Answer:\n",
      "1. Yes, the conversation did indicate which learning preferences the student may have.\n",
      "2. August's learning preferences include a preference for practical explanations and analogies over purely theoretical or formulaic descriptions.\n",
      "3. The parts of the conversation that indicated these learning preferences include:\n",
      "   - \"What are the formulas related to bayesian probs?\"\n",
      "   - \"Actually, I think I need these formulas explained in another way to understand bayesian probabilities\"\n",
      "   - \"Ahh it makes sense now, thank you!\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " 1. Yes, the conversation did indicate which learning preferences the student may have.\n",
      "2. August's learning preferences include a preference for practical explanations and analogies over purely theoretical or formulaic descriptions.\n",
      "3. The parts of the conversation that indicated these learning preferences include:\n",
      "   - \"What are the formulas related to bayesian probs?\"\n",
      "   - \"Actually, I think I need these formulas explained in another way to understand bayesian probabilities\"\n",
      "   - \"Ahh it makes sense now, thank you!\"\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello! How are you today?\\nTeaching Assistant: Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with your machine learning questions! What would you like to learn about today?\\nStudent: I would like to learn about probabilities relevant to machine learning\\nTeaching Assistant: In machine learning, probabilities are essential for handling uncertainty and making decisions based on data. Here are some key points on how probabilities are used in this field:\\n\\n1. **Probability Distributions**: Machine learning models often involve setting up probability distributions for all relevant quantities. These distributions help in understanding and modeling the uncertainty associated with data and model parameters.\\n\\n2. **Bayesian Probabilities**: Bayesian methods are particularly important as they provide a systematic way to update beliefs based on new evidence. This is done by applying Bayes' theorem, which updates the probability estimate for a hypothesis as more evidence or information becomes available.\\n\\n3. **Statistical Convergence**: Concepts like the central limit theorem explain why certain statistical properties (like the mean of a sample) converge to true values as more data points are collected. This is crucial in machine learning for validating model predictions and ensuring they are reliable as data size increases.\\n\\n4. **Empirical Estimates**: Probabilities can also be estimated directly from data. For example, the probability of observing a particular outcome can be estimated by the frequency of that outcome in the data. However, this approach can be challenging with large datasets and may not always provide accurate estimates, leading to the use of probabilistic models that summarize data with fewer parameters.\\n\\n5. **Handling Diverse Data Types**: Different types of data (continuous, discrete, categorical) require different probability distributions (e.g., Bernoulli, binomial, normal distributions) to model appropriately. Understanding these distributions and how they apply to various types of data is fundamental in machine learning.\\n\\nBy mastering these probabilistic concepts, you can better understand and implement machine learning algorithms, especially those that involve making predictions or classifications under uncertainty.\\nStudent: Can you expand on bayesian probabilities?\\nTeaching Assistant: Bayesian probabilities in machine learning involve a methodical approach to updating our beliefs about the likelihood of certain hypotheses based on new evidence. This process is central to many machine learning tasks, such as prediction and classification.\\n\\n1. **Probability Distributions**: Initially, a probability distribution is set up for all relevant quantities. This includes both the observable data and the parameters of the model. For example, if you're trying to determine whether a coin is biased (a 'magic coin'), you would start with a distribution that reflects your initial beliefs about the probability of getting heads or tails.\\n\\n2. **Bayes' Theorem**: This theorem is the cornerstone of Bayesian probability. It updates the probability estimate for a hypothesis as more data becomes available. Mathematically, it's expressed as:\\n   \\\\[\\n   p(\\\\theta | x) = \\\\frac{p(x | \\\\theta) p(\\\\theta)}{p(x)}\\n   \\\\]\\n   where \\\\( p(\\\\theta | x) \\\\) is the posterior probability of the hypothesis \\\\( \\\\theta \\\\) after observing data \\\\( x \\\\), \\\\( p(x | \\\\theta) \\\\) is the likelihood of observing the data given the hypothesis, \\\\( p(\\\\theta) \\\\) is the prior probability of the hypothesis, and \\\\( p(x) \\\\) is the probability of the data.\\n\\n3. **Application in Machine Learning**: In a machine learning context, Bayesian methods are used to continuously update the model as new data comes in. This is crucial in scenarios where data arrives incrementally or when the model needs to adapt to new patterns in the data. For instance, in spam detection, Bayesian classifiers update their beliefs about what constitutes spam as new examples are provided.\\n\\n4. **Bayesian Classifiers and Networks**: Bayesian classifiers make use of Bayes' theorem to classify data based on the likelihood of the data belonging to a particular class. Bayesian networks, or belief networks, extend this idea to multiple variables, creating a graphical model that represents conditional dependencies among variables.\\n\\nBy understanding and applying Bayesian probabilities, machine learning practitioners can create models that better adapt to new information and provide more accurate predictions or classifications based on the learned probabilities.\\nStudent: What are the formulas related to bayesian probs?\\nTeaching Assistant: The key formula related to Bayesian probabilities is Bayes' Theorem. Bayes' Theorem describes how to update the probability of a hypothesis based on new evidence. It is expressed mathematically as:\\n\\\\[\\np(\\\\theta | x) = \\\\frac{p(x | \\\\theta) \\\\cdot p(\\\\theta)}{p(x)}\\n\\\\]\\nWhere:\\n- \\\\(p(\\\\theta | x)\\\\) is the posterior probability of the hypothesis \\\\(\\\\theta\\\\) after observing the data \\\\(x\\\\).\\n- \\\\(p(x | \\\\theta)\\\\) is the likelihood, which is the probability of observing the data \\\\(x\\\\) given the hypothesis \\\\(\\\\theta\\\\).\\n- \\\\(p(\\\\theta)\\\\) is the prior probability of the hypothesis before seeing the data.\\n- \\\\(p(x)\\\\) is the marginal probability of the data under all possible hypotheses, often calculated as \\\\(\\\\sum_{\\\\theta} p(x | \\\\theta) \\\\cdot p(\\\\theta)\\\\) in discrete cases or integrated over all possible values of \\\\(\\\\theta\\\\) in continuous cases.\\n\\nThis formula is central to Bayesian inference and is widely used in various applications within machine learning, such as in Bayesian classifiers and Bayesian networks.\\nStudent: Actually, I think I need these formulas explained in another way to understand bayesian probabilities\\nTeaching Assistant: Bayesian probabilities can be thought of as a way to update our beliefs based on new evidence. Let's use an analogy to make this clearer:\\n\\nImagine you're a detective trying to figure out if a suspect is guilty of a crime. Initially, you have some belief about the likelihood of their guilt based on what you know about the case (this is your **prior** probability). As you gather more evidence (like fingerprints or witness statements), you need to update your belief about the suspect's guilt.\\n\\nHere's how Bayes' Theorem helps you in updating your belief:\\n1. **Prior ( \\\\( p(\\\\theta) \\\\) )**: This is your initial belief about the guilt of the suspect before seeing the new evidence. It's like your gut feeling based on previous information.\\n2. **Likelihood ( \\\\( p(x | \\\\theta) \\\\) )**: This is the probability of finding the evidence you have, assuming the suspect is guilty. For example, if guilty people usually leave fingerprints, how likely is it to find fingerprints at the crime scene if the suspect is guilty?\\n3. **Marginal Probability ( \\\\( p(x) \\\\) )**: This is the probability of observing the evidence under all possible scenarios (both guilty and not guilty). It acts as a normalizing factor to ensure that the probabilities add up to 1.\\n4. **Posterior ( \\\\( p(\\\\theta | x) \\\\) )**: This is your updated belief about the suspect's guilt after considering the new evidence. It combines your prior belief with how likely the evidence would appear if the suspect were guilty.\\n\\nMathematically, Bayes' Theorem is expressed as:\\n\\\\[\\np(\\\\theta | x) = \\\\frac{p(x | \\\\theta) \\\\cdot p(\\\\theta)}{p(x)}\\n\\\\]\\nThis formula tells you how to adjust your initial belief (prior) in light of the new evidence (likelihood) to get a new, updated belief (posterior).\\n\\nBy continuously updating your beliefs as new evidence comes in, you can make more informed decisions about the suspect's guilt, much like how machine learning models update their parameters as new data arrives.\\nStudent: Ahh it makes sense now, thank you!\\nTeaching Assistant: It's wonderful to hear that the information was helpful! If you have any more questions or need further details on Bayesian probabilities or any other aspect of machine learning, please don't hesitate to ask.\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe conversation between the student, August, and the Teaching Assistant revolves around the topic of Bayesian probabilities in machine learning. August initially asks for an explanation of probabilities relevant to machine learning and then specifically requests more information on Bayesian probabilities.\n",
      "\n",
      "August's request for the formulas related to Bayesian probabilities and subsequent request for an alternative explanation suggest a preference for understanding concepts through varied explanatory methods. This indicates a learning preference towards multiple forms of content delivery, particularly when dealing with complex mathematical concepts.\n",
      "\n",
      "Final Answer:\n",
      "{\n",
      "  \"found_preferences\": \"Yes\",\n",
      "  \"learning_preferences\": [\"Varied forms of content delivery\"],\n",
      "  \"indications\": [\n",
      "    \"Student: What are the formulas related to bayesian probs?\",\n",
      "    \"Student: Actually, I think I need these formulas explained in another way to understand bayesian probabilities\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " {\n",
      "  \"found_preferences\": \"Yes\",\n",
      "  \"learning_preferences\": [\"Varied forms of content delivery\"],\n",
      "  \"indications\": [\n",
      "    \"Student: What are the formulas related to bayesian probs?\",\n",
      "    \"Student: Actually, I think I need these formulas explained in another way to understand bayesian probabilities\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello! How are you today?\\nTeaching Assistant: Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with your machine learning questions! What would you like to learn about today?\\nStudent: I would like to learn about probabilities relevant to machine learning\\nTeaching Assistant: In machine learning, probabilities are essential for handling uncertainty and making decisions based on data. Here are some key points on how probabilities are used in this field:\\n\\n1. **Probability Distributions**: Machine learning models often involve setting up probability distributions for all relevant quantities. These distributions help in understanding and modeling the uncertainty associated with data and model parameters.\\n\\n2. **Bayesian Probabilities**: Bayesian methods are particularly important as they provide a systematic way to update beliefs based on new evidence. This is done by applying Bayes' theorem, which updates the probability estimate for a hypothesis as more evidence or information becomes available.\\n\\n3. **Statistical Convergence**: Concepts like the central limit theorem explain why certain statistical properties (like the mean of a sample) converge to true values as more data points are collected. This is crucial in machine learning for validating model predictions and ensuring they are reliable as data size increases.\\n\\n4. **Empirical Estimates**: Probabilities can also be estimated directly from data. For example, the probability of observing a particular outcome can be estimated by the frequency of that outcome in the data. However, this approach can be challenging with large datasets and may not always provide accurate estimates, leading to the use of probabilistic models that summarize data with fewer parameters.\\n\\n5. **Handling Diverse Data Types**: Different types of data (continuous, discrete, categorical) require different probability distributions (e.g., Bernoulli, binomial, normal distributions) to model appropriately. Understanding these distributions and how they apply to various types of data is fundamental in machine learning.\\n\\nBy mastering these probabilistic concepts, you can better understand and implement machine learning algorithms, especially those that involve making predictions or classifications under uncertainty.\\nStudent: Can you expand on bayesian probabilities?\\nTeaching Assistant: Bayesian probabilities in machine learning involve a methodical approach to updating our beliefs about the likelihood of certain hypotheses based on new evidence. This process is central to many machine learning tasks, such as prediction and classification.\\n\\n1. **Probability Distributions**: Initially, a probability distribution is set up for all relevant quantities. This includes both the observable data and the parameters of the model. For example, if you're trying to determine whether a coin is biased (a 'magic coin'), you would start with a distribution that reflects your initial beliefs about the probability of getting heads or tails.\\n\\n2. **Bayes' Theorem**: This theorem is the cornerstone of Bayesian probability. It updates the probability estimate for a hypothesis as more data becomes available. Mathematically, it's expressed as:\\n   \\\\[\\n   p(\\\\theta | x) = \\\\frac{p(x | \\\\theta) p(\\\\theta)}{p(x)}\\n   \\\\]\\n   where \\\\( p(\\\\theta | x) \\\\) is the posterior probability of the hypothesis \\\\( \\\\theta \\\\) after observing data \\\\( x \\\\), \\\\( p(x | \\\\theta) \\\\) is the likelihood of observing the data given the hypothesis, \\\\( p(\\\\theta) \\\\) is the prior probability of the hypothesis, and \\\\( p(x) \\\\) is the probability of the data.\\n\\n3. **Application in Machine Learning**: In a machine learning context, Bayesian methods are used to continuously update the model as new data comes in. This is crucial in scenarios where data arrives incrementally or when the model needs to adapt to new patterns in the data. For instance, in spam detection, Bayesian classifiers update their beliefs about what constitutes spam as new examples are provided.\\n\\n4. **Bayesian Classifiers and Networks**: Bayesian classifiers make use of Bayes' theorem to classify data based on the likelihood of the data belonging to a particular class. Bayesian networks, or belief networks, extend this idea to multiple variables, creating a graphical model that represents conditional dependencies among variables.\\n\\nBy understanding and applying Bayesian probabilities, machine learning practitioners can create models that better adapt to new information and provide more accurate predictions or classifications based on the learned probabilities.\\nStudent: What are the formulas related to bayesian probs?\\nTeaching Assistant: The key formula related to Bayesian probabilities is Bayes' Theorem. Bayes' Theorem describes how to update the probability of a hypothesis based on new evidence. It is expressed mathematically as:\\n\\\\[\\np(\\\\theta | x) = \\\\frac{p(x | \\\\theta) \\\\cdot p(\\\\theta)}{p(x)}\\n\\\\]\\nWhere:\\n- \\\\(p(\\\\theta | x)\\\\) is the posterior probability of the hypothesis \\\\(\\\\theta\\\\) after observing the data \\\\(x\\\\).\\n- \\\\(p(x | \\\\theta)\\\\) is the likelihood, which is the probability of observing the data \\\\(x\\\\) given the hypothesis \\\\(\\\\theta\\\\).\\n- \\\\(p(\\\\theta)\\\\) is the prior probability of the hypothesis before seeing the data.\\n- \\\\(p(x)\\\\) is the marginal probability of the data under all possible hypotheses, often calculated as \\\\(\\\\sum_{\\\\theta} p(x | \\\\theta) \\\\cdot p(\\\\theta)\\\\) in discrete cases or integrated over all possible values of \\\\(\\\\theta\\\\) in continuous cases.\\n\\nThis formula is central to Bayesian inference and is widely used in various applications within machine learning, such as in Bayesian classifiers and Bayesian networks.\\nStudent: Actually, I think I need these formulas explained in another way to understand bayesian probabilities\\nTeaching Assistant: Bayesian probabilities can be thought of as a way to update our beliefs based on new evidence. Let's use an analogy to make this clearer:\\n\\nImagine you're a detective trying to figure out if a suspect is guilty of a crime. Initially, you have some belief about the likelihood of their guilt based on what you know about the case (this is your **prior** probability). As you gather more evidence (like fingerprints or witness statements), you need to update your belief about the suspect's guilt.\\n\\nHere's how Bayes' Theorem helps you in updating your belief:\\n1. **Prior ( \\\\( p(\\\\theta) \\\\) )**: This is your initial belief about the guilt of the suspect before seeing the new evidence. It's like your gut feeling based on previous information.\\n2. **Likelihood ( \\\\( p(x | \\\\theta) \\\\) )**: This is the probability of finding the evidence you have, assuming the suspect is guilty. For example, if guilty people usually leave fingerprints, how likely is it to find fingerprints at the crime scene if the suspect is guilty?\\n3. **Marginal Probability ( \\\\( p(x) \\\\) )**: This is the probability of observing the evidence under all possible scenarios (both guilty and not guilty). It acts as a normalizing factor to ensure that the probabilities add up to 1.\\n4. **Posterior ( \\\\( p(\\\\theta | x) \\\\) )**: This is your updated belief about the suspect's guilt after considering the new evidence. It combines your prior belief with how likely the evidence would appear if the suspect were guilty.\\n\\nMathematically, Bayes' Theorem is expressed as:\\n\\\\[\\np(\\\\theta | x) = \\\\frac{p(x | \\\\theta) \\\\cdot p(\\\\theta)}{p(x)}\\n\\\\]\\nThis formula tells you how to adjust your initial belief (prior) in light of the new evidence (likelihood) to get a new, updated belief (posterior).\\n\\nBy continuously updating your beliefs as new evidence comes in, you can make more informed decisions about the suspect's guilt, much like how machine learning models update their parameters as new data arrives.\\nStudent: Ahh it makes sense now, thank you!\\nTeaching Assistant: It's wonderful to hear that the information was helpful! If you have any more questions or need further details on Bayesian probabilities or any other aspect of machine learning, please don't hesitate to ask.\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFrom the conversation, it appears that the student, August, prefers learning through examples and analogies when dealing with complex concepts like Bayesian probabilities. This preference is indicated when the student specifically requests the formulas to be explained \"in another way\" after the Teaching Assistant initially provides a mathematical explanation. The analogy of a detective updating beliefs about a suspect's guilt seemed to help the student understand Bayesian probabilities better, as evidenced by the student's positive response, \"Ahh it makes sense now, thank you!\"\n",
      "\n",
      "Final Answer:\n",
      "{\n",
      "  \"found_preferences\": \"Yes\",\n",
      "  \"learning_preferences\": [\"Examples and analogies\"],\n",
      "  \"indications\": [\n",
      "    \"Student: What are the formulas related to bayesian probs?\",\n",
      "    \"Teaching Assistant: The key formula related to Bayesian probabilities is Bayes' Theorem. Bayes' Theorem describes how to update the probability of a hypothesis based on new evidence. It is expressed mathematically as: [...]\",\n",
      "    \"Student: Actually, I think I need these formulas explained in another way to understand bayesian probabilities\",\n",
      "    \"Teaching Assistant: Bayesian probabilities can be thought of as a way to update our beliefs based on new evidence. Let's use an analogy to make this clearer: [...]\",\n",
      "    \"Student: Ahh it makes sense now, thank you!\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " {\n",
      "  \"found_preferences\": \"Yes\",\n",
      "  \"learning_preferences\": [\"Examples and analogies\"],\n",
      "  \"indications\": [\n",
      "    \"Student: What are the formulas related to bayesian probs?\",\n",
      "    \"Teaching Assistant: The key formula related to Bayesian probabilities is Bayes' Theorem. Bayes' Theorem describes how to update the probability of a hypothesis based on new evidence. It is expressed mathematically as: [...]\",\n",
      "    \"Student: Actually, I think I need these formulas explained in another way to understand bayesian probabilities\",\n",
      "    \"Teaching Assistant: Bayesian probabilities can be thought of as a way to update our beliefs based on new evidence. Let's use an analogy to make this clearer: [...]\",\n",
      "    \"Student: Ahh it makes sense now, thank you!\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello! How are you today?\\nTeaching Assistant: Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with your machine learning questions! What would you like to learn about today?\\nStudent: I would like to learn about probabilities relevant to machine learning\\nTeaching Assistant: In machine learning, probabilities are essential for handling uncertainty and making decisions based on data. Here are some key points on how probabilities are used in this field:\\n\\n1. **Probability Distributions**: Machine learning models often involve setting up probability distributions for all relevant quantities. These distributions help in understanding and modeling the uncertainty associated with data and model parameters.\\n\\n2. **Bayesian Probabilities**: Bayesian methods are particularly important as they provide a systematic way to update beliefs based on new evidence. This is done by applying Bayes' theorem, which updates the probability estimate for a hypothesis as more evidence or information becomes available.\\n\\n3. **Statistical Convergence**: Concepts like the central limit theorem explain why certain statistical properties (like the mean of a sample) converge to true values as more data points are collected. This is crucial in machine learning for validating model predictions and ensuring they are reliable as data size increases.\\n\\n4. **Empirical Estimates**: Probabilities can also be estimated directly from data. For example, the probability of observing a particular outcome can be estimated by the frequency of that outcome in the data. However, this approach can be challenging with large datasets and may not always provide accurate estimates, leading to the use of probabilistic models that summarize data with fewer parameters.\\n\\n5. **Handling Diverse Data Types**: Different types of data (continuous, discrete, categorical) require different probability distributions (e.g., Bernoulli, binomial, normal distributions) to model appropriately. Understanding these distributions and how they apply to various types of data is fundamental in machine learning.\\n\\nBy mastering these probabilistic concepts, you can better understand and implement machine learning algorithms, especially those that involve making predictions or classifications under uncertainty.\\nStudent: Can you expand on bayesian probabilities?\\nTeaching Assistant: Bayesian probabilities in machine learning involve a methodical approach to updating our beliefs about the likelihood of certain hypotheses based on new evidence. This process is central to many machine learning tasks, such as prediction and classification.\\n\\n1. **Probability Distributions**: Initially, a probability distribution is set up for all relevant quantities. This includes both the observable data and the parameters of the model. For example, if you're trying to determine whether a coin is biased (a 'magic coin'), you would start with a distribution that reflects your initial beliefs about the probability of getting heads or tails.\\n\\n2. **Bayes' Theorem**: This theorem is the cornerstone of Bayesian probability. It updates the probability estimate for a hypothesis as more data becomes available. Mathematically, it's expressed as:\\n   \\\\[\\n   p(\\\\theta | x) = \\\\frac{p(x | \\\\theta) p(\\\\theta)}{p(x)}\\n   \\\\]\\n   where \\\\( p(\\\\theta | x) \\\\) is the posterior probability of the hypothesis \\\\( \\\\theta \\\\) after observing data \\\\( x \\\\), \\\\( p(x | \\\\theta) \\\\) is the likelihood of observing the data given the hypothesis, \\\\( p(\\\\theta) \\\\) is the prior probability of the hypothesis, and \\\\( p(x) \\\\) is the probability of the data.\\n\\n3. **Application in Machine Learning**: In a machine learning context, Bayesian methods are used to continuously update the model as new data comes in. This is crucial in scenarios where data arrives incrementally or when the model needs to adapt to new patterns in the data. For instance, in spam detection, Bayesian classifiers update their beliefs about what constitutes spam as new examples are provided.\\n\\n4. **Bayesian Classifiers and Networks**: Bayesian classifiers make use of Bayes' theorem to classify data based on the likelihood of the data belonging to a particular class. Bayesian networks, or belief networks, extend this idea to multiple variables, creating a graphical model that represents conditional dependencies among variables.\\n\\nBy understanding and applying Bayesian probabilities, machine learning practitioners can create models that better adapt to new information and provide more accurate predictions or classifications based on the learned probabilities.\\nStudent: What are the formulas related to bayesian probs?\\nTeaching Assistant: The key formula related to Bayesian probabilities is Bayes' Theorem. Bayes' Theorem describes how to update the probability of a hypothesis based on new evidence. It is expressed mathematically as:\\n\\\\[\\np(\\\\theta | x) = \\\\frac{p(x | \\\\theta) \\\\cdot p(\\\\theta)}{p(x)}\\n\\\\]\\nWhere:\\n- \\\\(p(\\\\theta | x)\\\\) is the posterior probability of the hypothesis \\\\(\\\\theta\\\\) after observing the data \\\\(x\\\\).\\n- \\\\(p(x | \\\\theta)\\\\) is the likelihood, which is the probability of observing the data \\\\(x\\\\) given the hypothesis \\\\(\\\\theta\\\\).\\n- \\\\(p(\\\\theta)\\\\) is the prior probability of the hypothesis before seeing the data.\\n- \\\\(p(x)\\\\) is the marginal probability of the data under all possible hypotheses, often calculated as \\\\(\\\\sum_{\\\\theta} p(x | \\\\theta) \\\\cdot p(\\\\theta)\\\\) in discrete cases or integrated over all possible values of \\\\(\\\\theta\\\\) in continuous cases.\\n\\nThis formula is central to Bayesian inference and is widely used in various applications within machine learning, such as in Bayesian classifiers and Bayesian networks.\\nStudent: Actually, I think I need these formulas explained in another way to understand bayesian probabilities\\nTeaching Assistant: Bayesian probabilities can be thought of as a way to update our beliefs based on new evidence. Let's use an analogy to make this clearer:\\n\\nImagine you're a detective trying to figure out if a suspect is guilty of a crime. Initially, you have some belief about the likelihood of their guilt based on what you know about the case (this is your **prior** probability). As you gather more evidence (like fingerprints or witness statements), you need to update your belief about the suspect's guilt.\\n\\nHere's how Bayes' Theorem helps you in updating your belief:\\n1. **Prior ( \\\\( p(\\\\theta) \\\\) )**: This is your initial belief about the guilt of the suspect before seeing the new evidence. It's like your gut feeling based on previous information.\\n2. **Likelihood ( \\\\( p(x | \\\\theta) \\\\) )**: This is the probability of finding the evidence you have, assuming the suspect is guilty. For example, if guilty people usually leave fingerprints, how likely is it to find fingerprints at the crime scene if the suspect is guilty?\\n3. **Marginal Probability ( \\\\( p(x) \\\\) )**: This is the probability of observing the evidence under all possible scenarios (both guilty and not guilty). It acts as a normalizing factor to ensure that the probabilities add up to 1.\\n4. **Posterior ( \\\\( p(\\\\theta | x) \\\\) )**: This is your updated belief about the suspect's guilt after considering the new evidence. It combines your prior belief with how likely the evidence would appear if the suspect were guilty.\\n\\nMathematically, Bayes' Theorem is expressed as:\\n\\\\[\\np(\\\\theta | x) = \\\\frac{p(x | \\\\theta) \\\\cdot p(\\\\theta)}{p(x)}\\n\\\\]\\nThis formula tells you how to adjust your initial belief (prior) in light of the new evidence (likelihood) to get a new, updated belief (posterior).\\n\\nBy continuously updating your beliefs as new evidence comes in, you can make more informed decisions about the suspect's guilt, much like how machine learning models update their parameters as new data arrives.\\nStudent: Ahh it makes sense now, thank you!\\nTeaching Assistant: It's wonderful to hear that the information was helpful! If you have any more questions or need further details on Bayesian probabilities or any other aspect of machine learning, please don't hesitate to ask.\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe conversation between the student August and the Teaching Assistant revolves around the topic of Bayesian probabilities in the context of machine learning. August initially asks for an explanation of Bayesian probabilities, which the Teaching Assistant provides in a detailed manner, including key concepts and the formula for Bayes' Theorem. However, August later requests the formulas to be explained in another way, indicating a preference for understanding complex concepts through analogies or simplified explanations.\n",
      "\n",
      "This preference is evident when the Teaching Assistant uses a detective analogy to explain Bayesian probabilities, which helps August finally understand the concept, as indicated by his positive response.\n",
      "\n",
      "Final Answer:\n",
      "{\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Prefers analogies and simplified explanations for complex concepts\"],\n",
      "  \"indications\": [\n",
      "    \"Student: What are the formulas related to bayesian probs?\",\n",
      "    \"Teaching Assistant: The key formula related to Bayesian probabilities is Bayes' Theorem...\",\n",
      "    \"Student: Actually, I think I need these formulas explained in another way to understand bayesian probabilities\",\n",
      "    \"Teaching Assistant: Bayesian probabilities can be thought of as a way to update our beliefs based on new evidence. Let's use an analogy...\",\n",
      "    \"Student: Ahh it makes sense now, thank you!\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " {\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Prefers analogies and simplified explanations for complex concepts\"],\n",
      "  \"indications\": [\n",
      "    \"Student: What are the formulas related to bayesian probs?\",\n",
      "    \"Teaching Assistant: The key formula related to Bayesian probabilities is Bayes' Theorem...\",\n",
      "    \"Student: Actually, I think I need these formulas explained in another way to understand bayesian probabilities\",\n",
      "    \"Teaching Assistant: Bayesian probabilities can be thought of as a way to update our beliefs based on new evidence. Let's use an analogy...\",\n",
      "    \"Student: Ahh it makes sense now, thank you!\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello! How are you today?\\nTeaching Assistant: Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with your machine learning questions! What would you like to learn about today?\\nStudent: I would like to learn about probabilities relevant to machine learning\\nTeaching Assistant: In machine learning, probabilities are essential for handling uncertainty and making decisions based on data. Here are some key points on how probabilities are used in this field:\\n\\n1. **Probability Distributions**: Machine learning models often involve setting up probability distributions for all relevant quantities. These distributions help in understanding and modeling the uncertainty associated with data and model parameters.\\n\\n2. **Bayesian Probabilities**: Bayesian methods are particularly important as they provide a systematic way to update beliefs based on new evidence. This is done by applying Bayes' theorem, which updates the probability estimate for a hypothesis as more evidence or information becomes available.\\n\\n3. **Statistical Convergence**: Concepts like the central limit theorem explain why certain statistical properties (like the mean of a sample) converge to true values as more data points are collected. This is crucial in machine learning for validating model predictions and ensuring they are reliable as data size increases.\\n\\n4. **Empirical Estimates**: Probabilities can also be estimated directly from data. For example, the probability of observing a particular outcome can be estimated by the frequency of that outcome in the data. However, this approach can be challenging with large datasets and may not always provide accurate estimates, leading to the use of probabilistic models that summarize data with fewer parameters.\\n\\n5. **Handling Diverse Data Types**: Different types of data (continuous, discrete, categorical) require different probability distributions (e.g., Bernoulli, binomial, normal distributions) to model appropriately. Understanding these distributions and how they apply to various types of data is fundamental in machine learning.\\n\\nBy mastering these probabilistic concepts, you can better understand and implement machine learning algorithms, especially those that involve making predictions or classifications under uncertainty.\\nStudent: Can you expand on bayesian probabilities?\\nTeaching Assistant: Bayesian probabilities in machine learning involve a methodical approach to updating our beliefs about the likelihood of certain hypotheses based on new evidence. This process is central to many machine learning tasks, such as prediction and classification.\\n\\n1. **Probability Distributions**: Initially, a probability distribution is set up for all relevant quantities. This includes both the observable data and the parameters of the model. For example, if you're trying to determine whether a coin is biased (a 'magic coin'), you would start with a distribution that reflects your initial beliefs about the probability of getting heads or tails.\\n\\n2. **Bayes' Theorem**: This theorem is the cornerstone of Bayesian probability. It updates the probability estimate for a hypothesis as more data becomes available. Mathematically, it's expressed as:\\n   \\\\[\\n   p(\\\\theta | x) = \\\\frac{p(x | \\\\theta) p(\\\\theta)}{p(x)}\\n   \\\\]\\n   where \\\\( p(\\\\theta | x) \\\\) is the posterior probability of the hypothesis \\\\( \\\\theta \\\\) after observing data \\\\( x \\\\), \\\\( p(x | \\\\theta) \\\\) is the likelihood of observing the data given the hypothesis, \\\\( p(\\\\theta) \\\\) is the prior probability of the hypothesis, and \\\\( p(x) \\\\) is the probability of the data.\\n\\n3. **Application in Machine Learning**: In a machine learning context, Bayesian methods are used to continuously update the model as new data comes in. This is crucial in scenarios where data arrives incrementally or when the model needs to adapt to new patterns in the data. For instance, in spam detection, Bayesian classifiers update their beliefs about what constitutes spam as new examples are provided.\\n\\n4. **Bayesian Classifiers and Networks**: Bayesian classifiers make use of Bayes' theorem to classify data based on the likelihood of the data belonging to a particular class. Bayesian networks, or belief networks, extend this idea to multiple variables, creating a graphical model that represents conditional dependencies among variables.\\n\\nBy understanding and applying Bayesian probabilities, machine learning practitioners can create models that better adapt to new information and provide more accurate predictions or classifications based on the learned probabilities.\\nStudent: What are the formulas related to bayesian probs?\\nTeaching Assistant: The key formula related to Bayesian probabilities is Bayes' Theorem. Bayes' Theorem describes how to update the probability of a hypothesis based on new evidence. It is expressed mathematically as:\\n\\\\[\\np(\\\\theta | x) = \\\\frac{p(x | \\\\theta) \\\\cdot p(\\\\theta)}{p(x)}\\n\\\\]\\nWhere:\\n- \\\\(p(\\\\theta | x)\\\\) is the posterior probability of the hypothesis \\\\(\\\\theta\\\\) after observing the data \\\\(x\\\\).\\n- \\\\(p(x | \\\\theta)\\\\) is the likelihood, which is the probability of observing the data \\\\(x\\\\) given the hypothesis \\\\(\\\\theta\\\\).\\n- \\\\(p(\\\\theta)\\\\) is the prior probability of the hypothesis before seeing the data.\\n- \\\\(p(x)\\\\) is the marginal probability of the data under all possible hypotheses, often calculated as \\\\(\\\\sum_{\\\\theta} p(x | \\\\theta) \\\\cdot p(\\\\theta)\\\\) in discrete cases or integrated over all possible values of \\\\(\\\\theta\\\\) in continuous cases.\\n\\nThis formula is central to Bayesian inference and is widely used in various applications within machine learning, such as in Bayesian classifiers and Bayesian networks.\\nStudent: Actually, I think I need these formulas explained in another way to understand bayesian probabilities\\nTeaching Assistant: Bayesian probabilities can be thought of as a way to update our beliefs based on new evidence. Let's use an analogy to make this clearer:\\n\\nImagine you're a detective trying to figure out if a suspect is guilty of a crime. Initially, you have some belief about the likelihood of their guilt based on what you know about the case (this is your **prior** probability). As you gather more evidence (like fingerprints or witness statements), you need to update your belief about the suspect's guilt.\\n\\nHere's how Bayes' Theorem helps you in updating your belief:\\n1. **Prior ( \\\\( p(\\\\theta) \\\\) )**: This is your initial belief about the guilt of the suspect before seeing the new evidence. It's like your gut feeling based on previous information.\\n2. **Likelihood ( \\\\( p(x | \\\\theta) \\\\) )**: This is the probability of finding the evidence you have, assuming the suspect is guilty. For example, if guilty people usually leave fingerprints, how likely is it to find fingerprints at the crime scene if the suspect is guilty?\\n3. **Marginal Probability ( \\\\( p(x) \\\\) )**: This is the probability of observing the evidence under all possible scenarios (both guilty and not guilty). It acts as a normalizing factor to ensure that the probabilities add up to 1.\\n4. **Posterior ( \\\\( p(\\\\theta | x) \\\\) )**: This is your updated belief about the suspect's guilt after considering the new evidence. It combines your prior belief with how likely the evidence would appear if the suspect were guilty.\\n\\nMathematically, Bayes' Theorem is expressed as:\\n\\\\[\\np(\\\\theta | x) = \\\\frac{p(x | \\\\theta) \\\\cdot p(\\\\theta)}{p(x)}\\n\\\\]\\nThis formula tells you how to adjust your initial belief (prior) in light of the new evidence (likelihood) to get a new, updated belief (posterior).\\n\\nBy continuously updating your beliefs as new evidence comes in, you can make more informed decisions about the suspect's guilt, much like how machine learning models update their parameters as new data arrives.\\nStudent: Ahh it makes sense now, thank you!\\nTeaching Assistant: It's wonderful to hear that the information was helpful! If you have any more questions or need further details on Bayesian probabilities or any other aspect of machine learning, please don't hesitate to ask.\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No clear preference, the student has not indicated a preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe conversation between the student, August, and the Teaching Assistant revolves around the topic of probabilities in machine learning, with a specific focus on Bayesian probabilities. August's interaction with the Teaching Assistant indicates a preference for detailed, structured explanations and a focus on understanding theoretical concepts.\n",
      "\n",
      "Thought: From the conversation, it appears that August is keen on understanding not just the basic concepts but also their application in machine learning. This can be inferred from the detailed questions and the follow-up on Bayesian probabilities. To confirm if there are specific learning preferences, I need to analyze the conversation further and identify any indications of preferred learning methods or styles.\n",
      "\n",
      "Action: Analyze the conversation for indications of learning preferences.\n",
      "\n",
      "Action Input: Review the conversation snippets provided.\n",
      "\u001b[0mAnalyze the conversation for indications of learning preferences. is not a valid tool, try one of [Web Search, Retrieval Tool].\u001b[32;1m\u001b[1;3mIn reviewing the conversation, August asks for an expansion on Bayesian probabilities after receiving a structured explanation on probabilities in machine learning. This indicates a preference for deep dives into specific topics and a theoretical understanding rather than just a superficial overview.\n",
      "\n",
      "Thought: August's request for more information on Bayesian probabilities after a detailed initial explanation suggests a preference for thorough understanding and possibly a visual or structured learning style, as the teaching assistant used bullet points and mathematical notation in the explanation. However, more data would be needed to confirm these preferences definitively.\n",
      "\n",
      "Final Answer: {\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Detailed explanations\", \"Theoretical understanding\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you expand on bayesian probabilities?\",\n",
      "    \"Teaching Assistant: Bayesian probabilities in machine learning involve a methodical approach to updating our beliefs about the likelihood of certain hypotheses based on new evidence...\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " {\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Detailed explanations\", \"Theoretical understanding\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you expand on bayesian probabilities?\",\n",
      "    \"Teaching Assistant: Bayesian probabilities in machine learning involve a methodical approach to updating our beliefs about the likelihood of certain hypotheses based on new evidence...\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello! How are you today?\\nTeaching Assistant: Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with your machine learning questions! What would you like to learn about today?\\nStudent: I would like to learn about probabilities relevant to machine learning\\nTeaching Assistant: In machine learning, probabilities are essential for handling uncertainty and making decisions based on data. Here are some key points on how probabilities are used in this field:\\n\\n1. **Probability Distributions**: Machine learning models often involve setting up probability distributions for all relevant quantities. These distributions help in understanding and modeling the uncertainty associated with data and model parameters.\\n\\n2. **Bayesian Probabilities**: Bayesian methods are particularly important as they provide a systematic way to update beliefs based on new evidence. This is done by applying Bayes' theorem, which updates the probability estimate for a hypothesis as more evidence or information becomes available.\\n\\n3. **Statistical Convergence**: Concepts like the central limit theorem explain why certain statistical properties (like the mean of a sample) converge to true values as more data points are collected. This is crucial in machine learning for validating model predictions and ensuring they are reliable as data size increases.\\n\\n4. **Empirical Estimates**: Probabilities can also be estimated directly from data. For example, the probability of observing a particular outcome can be estimated by the frequency of that outcome in the data. However, this approach can be challenging with large datasets and may not always provide accurate estimates, leading to the use of probabilistic models that summarize data with fewer parameters.\\n\\n5. **Handling Diverse Data Types**: Different types of data (continuous, discrete, categorical) require different probability distributions (e.g., Bernoulli, binomial, normal distributions) to model appropriately. Understanding these distributions and how they apply to various types of data is fundamental in machine learning.\\n\\nBy mastering these probabilistic concepts, you can better understand and implement machine learning algorithms, especially those that involve making predictions or classifications under uncertainty.\\nStudent: Can you expand on bayesian probabilities?\\nTeaching Assistant: Bayesian probabilities in machine learning involve a methodical approach to updating our beliefs about the likelihood of certain hypotheses based on new evidence. This process is central to many machine learning tasks, such as prediction and classification.\\n\\n1. **Probability Distributions**: Initially, a probability distribution is set up for all relevant quantities. This includes both the observable data and the parameters of the model. For example, if you're trying to determine whether a coin is biased (a 'magic coin'), you would start with a distribution that reflects your initial beliefs about the probability of getting heads or tails.\\n\\n2. **Bayes' Theorem**: This theorem is the cornerstone of Bayesian probability. It updates the probability estimate for a hypothesis as more data becomes available. Mathematically, it's expressed as:\\n   \\\\[\\n   p(\\\\theta | x) = \\\\frac{p(x | \\\\theta) p(\\\\theta)}{p(x)}\\n   \\\\]\\n   where \\\\( p(\\\\theta | x) \\\\) is the posterior probability of the hypothesis \\\\( \\\\theta \\\\) after observing data \\\\( x \\\\), \\\\( p(x | \\\\theta) \\\\) is the likelihood of observing the data given the hypothesis, \\\\( p(\\\\theta) \\\\) is the prior probability of the hypothesis, and \\\\( p(x) \\\\) is the probability of the data.\\n\\n3. **Application in Machine Learning**: In a machine learning context, Bayesian methods are used to continuously update the model as new data comes in. This is crucial in scenarios where data arrives incrementally or when the model needs to adapt to new patterns in the data. For instance, in spam detection, Bayesian classifiers update their beliefs about what constitutes spam as new examples are provided.\\n\\n4. **Bayesian Classifiers and Networks**: Bayesian classifiers make use of Bayes' theorem to classify data based on the likelihood of the data belonging to a particular class. Bayesian networks, or belief networks, extend this idea to multiple variables, creating a graphical model that represents conditional dependencies among variables.\\n\\nBy understanding and applying Bayesian probabilities, machine learning practitioners can create models that better adapt to new information and provide more accurate predictions or classifications based on the learned probabilities.\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe conversation between August and the teaching assistant primarily revolves around the explanation of probabilities and Bayesian probabilities in machine learning. August's request to expand on Bayesian probabilities after the initial explanation suggests a preference for deeper understanding of specific topics.\n",
      "\n",
      "Thought: To identify learning preferences, I need to analyze the type of questions August asks and how he responds to the information provided by the teaching assistant.\n",
      "\n",
      "Action: None required at this stage.\n",
      "\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3mAugust's request for an expansion on Bayesian probabilities indicates a preference for detailed and thorough explanations, especially when dealing with complex concepts. This suggests that he may benefit from learning approaches that involve step-by-step breakdowns of concepts and possibly visual aids that clarify the relationships and processes involved in Bayesian probabilities.\n",
      "\n",
      "Final Answer:\n",
      "{\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Detailed explanations\", \"Step-by-step breakdowns\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you expand on bayesian probabilities?\",\n",
      "    \"Teaching Assistant: Bayesian probabilities in machine learning involve a methodical approach to updating our beliefs about the likelihood of certain hypotheses based on new evidence. This process is central to many machine learning tasks, such as prediction and classification.\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " {\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Detailed explanations\", \"Step-by-step breakdowns\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you expand on bayesian probabilities?\",\n",
      "    \"Teaching Assistant: Bayesian probabilities in machine learning involve a methodical approach to updating our beliefs about the likelihood of certain hypotheses based on new evidence. This process is central to many machine learning tasks, such as prediction and classification.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello! How are you today?\\nTeaching Assistant: Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with your machine learning questions! What would you like to learn about today?\\nStudent: I would like to learn about probabilities relevant to machine learning\\nTeaching Assistant: In machine learning, probabilities are essential for handling uncertainty and making decisions based on data. Here are some key points on how probabilities are used in this field:\\n\\n1. **Probability Distributions**: Machine learning models often involve setting up probability distributions for all relevant quantities. These distributions help in understanding and modeling the uncertainty associated with data and model parameters.\\n\\n2. **Bayesian Probabilities**: Bayesian methods are particularly important as they provide a systematic way to update beliefs based on new evidence. This is done by applying Bayes' theorem, which updates the probability estimate for a hypothesis as more evidence or information becomes available.\\n\\n3. **Statistical Convergence**: Concepts like the central limit theorem explain why certain statistical properties (like the mean of a sample) converge to true values as more data points are collected. This is crucial in machine learning for validating model predictions and ensuring they are reliable as data size increases.\\n\\n4. **Empirical Estimates**: Probabilities can also be estimated directly from data. For example, the probability of observing a particular outcome can be estimated by the frequency of that outcome in the data. However, this approach can be challenging with large datasets and may not always provide accurate estimates, leading to the use of probabilistic models that summarize data with fewer parameters.\\n\\n5. **Handling Diverse Data Types**: Different types of data (continuous, discrete, categorical) require different probability distributions (e.g., Bernoulli, binomial, normal distributions) to model appropriately. Understanding these distributions and how they apply to various types of data is fundamental in machine learning.\\n\\nBy mastering these probabilistic concepts, you can better understand and implement machine learning algorithms, especially those that involve making predictions or classifications under uncertainty.\\nStudent: Can you expand on bayesian probabilities?\\nTeaching Assistant: Bayesian probabilities in machine learning involve a methodical approach to updating our beliefs about the likelihood of certain hypotheses based on new evidence. This process is central to many machine learning tasks, such as prediction and classification.\\n\\n1. **Probability Distributions**: Initially, a probability distribution is set up for all relevant quantities. This includes both the observable data and the parameters of the model. For example, if you're trying to determine whether a coin is biased (a 'magic coin'), you would start with a distribution that reflects your initial beliefs about the probability of getting heads or tails.\\n\\n2. **Bayes' Theorem**: This theorem is the cornerstone of Bayesian probability. It updates the probability estimate for a hypothesis as more data becomes available. Mathematically, it's expressed as:\\n   \\\\[\\n   p(\\\\theta | x) = \\\\frac{p(x | \\\\theta) p(\\\\theta)}{p(x)}\\n   \\\\]\\n   where \\\\( p(\\\\theta | x) \\\\) is the posterior probability of the hypothesis \\\\( \\\\theta \\\\) after observing data \\\\( x \\\\), \\\\( p(x | \\\\theta) \\\\) is the likelihood of observing the data given the hypothesis, \\\\( p(\\\\theta) \\\\) is the prior probability of the hypothesis, and \\\\( p(x) \\\\) is the probability of the data.\\n\\n3. **Application in Machine Learning**: In a machine learning context, Bayesian methods are used to continuously update the model as new data comes in. This is crucial in scenarios where data arrives incrementally or when the model needs to adapt to new patterns in the data. For instance, in spam detection, Bayesian classifiers update their beliefs about what constitutes spam as new examples are provided.\\n\\n4. **Bayesian Classifiers and Networks**: Bayesian classifiers make use of Bayes' theorem to classify data based on the likelihood of the data belonging to a particular class. Bayesian networks, or belief networks, extend this idea to multiple variables, creating a graphical model that represents conditional dependencies among variables.\\n\\nBy understanding and applying Bayesian probabilities, machine learning practitioners can create models that better adapt to new information and provide more accurate predictions or classifications based on the learned probabilities.\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual learning preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mIn this conversation, August is discussing classification trees and their implementation in Python, focusing specifically on how to compute outputs and understand entropy in the context of decision trees. The conversation revolves around detailed explanations and practical implementation, which could indicate certain learning preferences.\n",
      "\n",
      "To analyze August's learning preferences, I will examine the conversation for any clues about how August prefers to receive information and what explanations or methods help him understand the topic better.\n",
      "\n",
      "Thought: Review the conversation snippets to identify any specific learning preferences August might have shown, focusing on his reactions to explanations and the types of questions he asks.\n",
      "\n",
      "Action: Review the conversation snippets provided.\n",
      "\n",
      "Action Input: N/A\n",
      "\u001b[0mReview the conversation snippets provided. is not a valid tool, try one of [Web Search, Retrieval Tool].\u001b[32;1m\u001b[1;3mFrom the conversation, it's evident that August engages in a detailed discussion about classification trees, asking for examples, code implementations, and explanations of concepts like entropy. His questions are specific and directed towards understanding practical application and theoretical concepts. This suggests certain learning preferences.\n",
      "\n",
      "August's request for a Python implementation and a specific example problem indicates a preference for practical, hands-on learning and visual aids (like the tree plot in Python). His follow-up question about the computation of entropy and its significance in decision trees suggests he benefits from detailed theoretical explanations that connect concepts with practical applications.\n",
      "\n",
      "Final Answer:\n",
      "{\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Hands-on learning\", \"Visual aids\", \"Detailed theoretical explanations\"],\n",
      "  \"indications\": [\n",
      "    \"Student: How would I implement this in Python?\",\n",
      "    \"Teaching Assistant: Here's a Python implementation to solve the classification tree problem for the given dataset:...\",\n",
      "    \"Student: Can you explain what is meant by entropy in the plot? and how this is computed\",\n",
      "    \"Teaching Assistant: In the context of decision trees, entropy is used to measure the impurity or randomness in the dataset at each node...\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " {\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Hands-on learning\", \"Visual aids\", \"Detailed theoretical explanations\"],\n",
      "  \"indications\": [\n",
      "    \"Student: How would I implement this in Python?\",\n",
      "    \"Teaching Assistant: Here's a Python implementation to solve the classification tree problem for the given dataset:...\",\n",
      "    \"Student: Can you explain what is meant by entropy in the plot? and how this is computed\",\n",
      "    \"Teaching Assistant: In the context of decision trees, entropy is used to measure the impurity or randomness in the dataset at each node...\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello, I am August, studying at the technical university of denmark, in the course Introduction to Machine Learning\\nTeaching Assistant: Hi August! Welcome to the course on Introduction to Machine Learning. How can I assist you today? Are there specific topics or concepts within machine learning that you would like to discuss or need help understanding?\\nStudent: I am studying for the written exam, in which I have to be able to apply some of the methods to problems. Right now I am having trouble with understanding classification trees, specifically how I can compute an output from these in hand\\nTeaching Assistant: To compute an output from a classification tree by hand, you follow these steps:\\n\\n1. **Start at the Root Node**: Begin at the top of the tree. This node contains a condition or question about one of the features of your data. For instance, in the provided dataset, a question at the root might be \\\"Does the animal have fur?\\\"\\n\\n2. **Follow the Branches Based on Features**: Depending on the answer to the question (Yes or No), follow the branch down to the next node. Each node in the tree represents another question about a feature until you reach a leaf node.\\n\\n3. **Reach a Leaf Node**: Leaf nodes represent the classification outcome based on the path of decisions made from the root to that leaf. For example, if you followed the path for an animal that has fur, you might end up at a leaf node classifying the animal as a mammal.\\n\\n4. **Example Calculation**:\\n   - Consider a new example, say a 'Bear' with features: Cold Blooded (no), Has Legs (yes), Lay Eggs (no), Has Fur (yes).\\n   - Starting at the root, if the first question is about having fur, you would answer yes and follow the branch to the next node.\\n   - If the subsequent nodes ask about laying eggs or being cold-blooded, your answers (no, no) will guide you through specific branches.\\n   - Finally, you would reach a leaf node that classifies the bear as a mammal based on the answers provided.\\n\\nThis method of traversing from the root to a leaf, based on feature responses, is how you manually compute an output from a classification tree. Each decision point (node) splits the data further based on the feature until a final classification is made at the leaves.\\nStudent: Can you show me an example exam problem?\\nTeaching Assistant: **Example Exam Problem: Classification Trees**\\n\\n**Problem Description:**\\nYou are provided with a dataset of 15 animals, each characterized by four binary features: Cold Blooded (x1), Has Legs (x2), Lay Eggs (x3), and Has Fur (x4). The goal is to predict whether an animal is a Mammal (y) or not based on these features.\\n\\n**Dataset:**\\n- Snake: (yes, no, yes, no)\\n- Starfish: (yes, no, yes, no)\\n- Bluebird: (no, yes, yes, no)\\n- Blackbird: (no, yes, yes, no)\\n- Earthworm: (yes, no, yes, no)\\n- Chameleon: (yes, no, yes, no)\\n- Ant: (yes, no, yes, no)\\n- Jellyfish: (yes, no, yes, no)\\n- Snail: (yes, no, yes, no)\\n- Sea Urchin: (yes, no, yes, no)\\n- Dolphin: (no, yes, no, yes)\\n- Rat: (no, yes, no, yes)\\n- Dog: (no, yes, no, yes)\\n- Monkey: (no, yes, no, yes)\\n- Lion: (no, yes, no, yes)\\n\\n**Task:**\\n1. Using the dataset provided, construct a classification tree to predict the 'Mammal' label. Start by choosing a feature that best splits the data at the root, and continue splitting each subset using the remaining features until all nodes are pure or no further splitting is possible.\\n2. Explain the rationale behind each split you choose and how it helps in classifying the animals.\\n3. Draw the resulting classification tree.\\n\\n**Hints:**\\n- Consider the purity of nodes after each split. A node is pure if all its instances belong to a single class (either all mammals or all non-mammals).\\n- Use binary questions (yes/no) based on the features to split the data at each node.\\n\\nThis problem will help you understand how to manually build a classification tree and how to use it for making predictions based on the features of new instances.\\nStudent: How would I implement this in Python?\\nTeaching Assistant: To assist you best, could you please specify what you would like to implement in Python? Are you looking to create a classification tree for the example exam problem we discussed, or is there another specific aspect of classification trees or machine learning that you need help implementing? This will help me provide you with the most relevant code and explanation.\\nStudent: For the example exam problem\\nTeaching Assistant: Here's a Python implementation to solve the classification tree problem for the given dataset:\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\nimport matplotlib.pyplot as plt\\n\\n# Create the dataset\\ndata = {\\n    'Cold Blooded': [1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\\n    'Has Legs': [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\\n    'Lay Eggs': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\\n    'Has Fur': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\\n    'Mammal': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\\n}\\ndf = pd.DataFrame(data)\\n\\n# Features and target\\nX = df[['Cold Blooded', 'Has Legs', 'Lay Eggs', 'Has Fur']]\\ny = df['Mammal']\\n\\n# Create and train the classifier\\nclassifier = DecisionTreeClassifier(criterion='entropy')\\nclassifier.fit(X, y)\\n\\n# Visualize the tree\\nplt.figure(figsize=(12,8))\\ntree.plot_tree(classifier, filled=True, feature_names=['Cold Blooded', 'Has Legs', 'Lay Eggs', 'Has Fur'], class_names=['Not Mammal', 'Mammal'])\\nplt.show()\\n```\\n\\nThis code will:\\n1. Create a DataFrame with the given dataset.\\n2. Separate the features and the target label.\\n3. Train a Decision Tree Classifier using the entropy criterion to manage splits based on information gain.\\n4. Visualize the tree to see how the features split the data and how decisions are made at each node.\\n\\nThis visualization will help you understand the structure of the classification tree and how it classifies the animals based on their features.\\nStudent: Can you explain what is meant by entropy in the plot? and how this is computed\\nTeaching Assistant: In the context of decision trees, entropy is used to measure the impurity or randomness in the dataset at each node. It is computed using the formula $H(S) = -\\\\sum_{i=1}^{n} p_i \\\\log_2 p_i$, where $p_i$ represents the proportion of class $i$ instances at the node. The aim in using entropy is to choose the splits that result in the most significant decrease in entropy, leading to branches with higher purity. This method helps in building an efficient and effective classification tree.\\nStudent: Ahh, so in the plot, the largest decrease in entropy happens when each animal is categorized on whether they have fur or not?\\nTeaching Assistant: Yes, in the plot of a decision tree for classifying animals, the feature 'Has Fur' can result in the largest decrease in entropy when used to categorize animals, especially if it distinctly separates mammals from non-mammals. This significant reduction in entropy is due to the feature creating highly pure (homogeneous) branches, which is ideal for effective classification.\\nStudent: Ok I think I understand it now\\nTeaching Assistant: That's great to hear, August! If you feel comfortable with the topic of classification trees and their implementation, do you have any other questions or is there another topic within machine learning that you would like to explore? I'm here to help with any aspect of your studies!\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mBased on the conversation between August and the Teaching Assistant, it appears that August has a preference for practical, example-based learning and possibly appreciates visual aids.\n",
      "\n",
      "Thought: August's request for an example exam problem and a Python implementation for a specific example suggests a preference for learning through practical application. Additionally, his follow-up questions about the implementation details and the concept of entropy indicate a desire to understand underlying concepts through specific examples. Let's structure the answer based on these observations.\n",
      "\n",
      "Final Answer:\n",
      "{\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Practical application\", \"Visual learning\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you show me an example exam problem?\",\n",
      "    \"Student: How would I implement this in Python?\",\n",
      "    \"Teaching Assistant: Here's a Python implementation to solve the classification tree problem for the given dataset...\",\n",
      "    \"Student: Can you explain what is meant by entropy in the plot? and how this is computed\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " {\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Practical application\", \"Visual learning\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you show me an example exam problem?\",\n",
      "    \"Student: How would I implement this in Python?\",\n",
      "    \"Teaching Assistant: Here's a Python implementation to solve the classification tree problem for the given dataset...\",\n",
      "    \"Student: Can you explain what is meant by entropy in the plot? and how this is computed\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello, I am August, studying at the technical university of denmark, in the course Introduction to Machine Learning\\nTeaching Assistant: Hi August! Welcome to the course on Introduction to Machine Learning. How can I assist you today? Are there specific topics or concepts within machine learning that you would like to discuss or need help understanding?\\nStudent: I am studying for the written exam, in which I have to be able to apply some of the methods to problems. Right now I am having trouble with understanding classification trees, specifically how I can compute an output from these in hand\\nTeaching Assistant: To compute an output from a classification tree by hand, you follow these steps:\\n\\n1. **Start at the Root Node**: Begin at the top of the tree. This node contains a condition or question about one of the features of your data. For instance, in the provided dataset, a question at the root might be \\\"Does the animal have fur?\\\"\\n\\n2. **Follow the Branches Based on Features**: Depending on the answer to the question (Yes or No), follow the branch down to the next node. Each node in the tree represents another question about a feature until you reach a leaf node.\\n\\n3. **Reach a Leaf Node**: Leaf nodes represent the classification outcome based on the path of decisions made from the root to that leaf. For example, if you followed the path for an animal that has fur, you might end up at a leaf node classifying the animal as a mammal.\\n\\n4. **Example Calculation**:\\n   - Consider a new example, say a 'Bear' with features: Cold Blooded (no), Has Legs (yes), Lay Eggs (no), Has Fur (yes).\\n   - Starting at the root, if the first question is about having fur, you would answer yes and follow the branch to the next node.\\n   - If the subsequent nodes ask about laying eggs or being cold-blooded, your answers (no, no) will guide you through specific branches.\\n   - Finally, you would reach a leaf node that classifies the bear as a mammal based on the answers provided.\\n\\nThis method of traversing from the root to a leaf, based on feature responses, is how you manually compute an output from a classification tree. Each decision point (node) splits the data further based on the feature until a final classification is made at the leaves.\\nStudent: Can you show me an example exam problem?\\nTeaching Assistant: **Example Exam Problem: Classification Trees**\\n\\n**Problem Description:**\\nYou are provided with a dataset of 15 animals, each characterized by four binary features: Cold Blooded (x1), Has Legs (x2), Lay Eggs (x3), and Has Fur (x4). The goal is to predict whether an animal is a Mammal (y) or not based on these features.\\n\\n**Dataset:**\\n- Snake: (yes, no, yes, no)\\n- Starfish: (yes, no, yes, no)\\n- Bluebird: (no, yes, yes, no)\\n- Blackbird: (no, yes, yes, no)\\n- Earthworm: (yes, no, yes, no)\\n- Chameleon: (yes, no, yes, no)\\n- Ant: (yes, no, yes, no)\\n- Jellyfish: (yes, no, yes, no)\\n- Snail: (yes, no, yes, no)\\n- Sea Urchin: (yes, no, yes, no)\\n- Dolphin: (no, yes, no, yes)\\n- Rat: (no, yes, no, yes)\\n- Dog: (no, yes, no, yes)\\n- Monkey: (no, yes, no, yes)\\n- Lion: (no, yes, no, yes)\\n\\n**Task:**\\n1. Using the dataset provided, construct a classification tree to predict the 'Mammal' label. Start by choosing a feature that best splits the data at the root, and continue splitting each subset using the remaining features until all nodes are pure or no further splitting is possible.\\n2. Explain the rationale behind each split you choose and how it helps in classifying the animals.\\n3. Draw the resulting classification tree.\\n\\n**Hints:**\\n- Consider the purity of nodes after each split. A node is pure if all its instances belong to a single class (either all mammals or all non-mammals).\\n- Use binary questions (yes/no) based on the features to split the data at each node.\\n\\nThis problem will help you understand how to manually build a classification tree and how to use it for making predictions based on the features of new instances.\\nStudent: How would I implement this in Python?\\nTeaching Assistant: To assist you best, could you please specify what you would like to implement in Python? Are you looking to create a classification tree for the example exam problem we discussed, or is there another specific aspect of classification trees or machine learning that you need help implementing? This will help me provide you with the most relevant code and explanation.\\nStudent: For the example exam problem\\nTeaching Assistant: Here's a Python implementation to solve the classification tree problem for the given dataset:\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\nimport matplotlib.pyplot as plt\\n\\n# Create the dataset\\ndata = {\\n    'Cold Blooded': [1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\\n    'Has Legs': [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\\n    'Lay Eggs': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\\n    'Has Fur': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\\n    'Mammal': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\\n}\\ndf = pd.DataFrame(data)\\n\\n# Features and target\\nX = df[['Cold Blooded', 'Has Legs', 'Lay Eggs', 'Has Fur']]\\ny = df['Mammal']\\n\\n# Create and train the classifier\\nclassifier = DecisionTreeClassifier(criterion='entropy')\\nclassifier.fit(X, y)\\n\\n# Visualize the tree\\nplt.figure(figsize=(12,8))\\ntree.plot_tree(classifier, filled=True, feature_names=['Cold Blooded', 'Has Legs', 'Lay Eggs', 'Has Fur'], class_names=['Not Mammal', 'Mammal'])\\nplt.show()\\n```\\n\\nThis code will:\\n1. Create a DataFrame with the given dataset.\\n2. Separate the features and the target label.\\n3. Train a Decision Tree Classifier using the entropy criterion to manage splits based on information gain.\\n4. Visualize the tree to see how the features split the data and how decisions are made at each node.\\n\\nThis visualization will help you understand the structure of the classification tree and how it classifies the animals based on their features.\\nStudent: Can you explain what is meant by entropy in the plot? and how this is computed\\nTeaching Assistant: In the context of decision trees, entropy is used to measure the impurity or randomness in the dataset at each node. It is computed using the formula $H(S) = -\\\\sum_{i=1}^{n} p_i \\\\log_2 p_i$, where $p_i$ represents the proportion of class $i$ instances at the node. The aim in using entropy is to choose the splits that result in the most significant decrease in entropy, leading to branches with higher purity. This method helps in building an efficient and effective classification tree.\\nStudent: Ahh, so in the plot, the largest decrease in entropy happens when each animal is categorized on whether they have fur or not?\\nTeaching Assistant: Yes, in the plot of a decision tree for classifying animals, the feature 'Has Fur' can result in the largest decrease in entropy when used to categorize animals, especially if it distinctly separates mammals from non-mammals. This significant reduction in entropy is due to the feature creating highly pure (homogeneous) branches, which is ideal for effective classification.\\nStudent: Ok I think I understand it now\\nTeaching Assistant: That's great to hear, August! If you feel comfortable with the topic of classification trees and their implementation, do you have any other questions or is there another topic within machine learning that you would like to explore? I'm here to help with any aspect of your studies!\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mBased on the conversation, it seems that August has a preference for practical examples and step-by-step explanations when learning about complex concepts like classification trees. This inference is drawn from the student's request for an example exam problem and a Python implementation to understand the application of the theory.\n",
      "\n",
      "Action: None needed\n",
      "\n",
      "Action Input: None\n",
      "\u001b[0mNone needed is not a valid tool, try one of [Web Search, Retrieval Tool].\u001b[32;1m\u001b[1;3mThe conversation indicates that August benefits from practical examples and clear, step-by-step explanations. This is evident from his request for an example exam problem and further clarification on how to implement classification trees in Python. His understanding was further solidified through the explanation of entropy in decision trees, which he specifically asked about and acknowledged understanding after the Teaching Assistant's explanation.\n",
      "\n",
      "Final Answer:\n",
      "{\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Practical examples\", \"Step-by-step explanations\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you show me an example exam problem?\",\n",
      "    \"Student: How would I implement this in Python? For the example exam problem\",\n",
      "    \"Student: Can you explain what is meant by entropy in the plot? and how this is computed\",\n",
      "    \"Student: Ahh, so in the plot, the largest decrease in entropy happens when each animal is categorized on whether they have fur or not?\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " {\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Practical examples\", \"Step-by-step explanations\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you show me an example exam problem?\",\n",
      "    \"Student: How would I implement this in Python? For the example exam problem\",\n",
      "    \"Student: Can you explain what is meant by entropy in the plot? and how this is computed\",\n",
      "    \"Student: Ahh, so in the plot, the largest decrease in entropy happens when each animal is categorized on whether they have fur or not?\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello, I am August, studying at the technical university of denmark, in the course Introduction to Machine Learning\\nTeaching Assistant: Hi August! Welcome to the course on Introduction to Machine Learning. How can I assist you today? Are there specific topics or concepts within machine learning that you would like to discuss or need help understanding?\\nStudent: I am studying for the written exam, in which I have to be able to apply some of the methods to problems. Right now I am having trouble with understanding classification trees, specifically how I can compute an output from these in hand\\nTeaching Assistant: To compute an output from a classification tree by hand, you follow these steps:\\n\\n1. **Start at the Root Node**: Begin at the top of the tree. This node contains a condition or question about one of the features of your data. For instance, in the provided dataset, a question at the root might be \\\"Does the animal have fur?\\\"\\n\\n2. **Follow the Branches Based on Features**: Depending on the answer to the question (Yes or No), follow the branch down to the next node. Each node in the tree represents another question about a feature until you reach a leaf node.\\n\\n3. **Reach a Leaf Node**: Leaf nodes represent the classification outcome based on the path of decisions made from the root to that leaf. For example, if you followed the path for an animal that has fur, you might end up at a leaf node classifying the animal as a mammal.\\n\\n4. **Example Calculation**:\\n   - Consider a new example, say a 'Bear' with features: Cold Blooded (no), Has Legs (yes), Lay Eggs (no), Has Fur (yes).\\n   - Starting at the root, if the first question is about having fur, you would answer yes and follow the branch to the next node.\\n   - If the subsequent nodes ask about laying eggs or being cold-blooded, your answers (no, no) will guide you through specific branches.\\n   - Finally, you would reach a leaf node that classifies the bear as a mammal based on the answers provided.\\n\\nThis method of traversing from the root to a leaf, based on feature responses, is how you manually compute an output from a classification tree. Each decision point (node) splits the data further based on the feature until a final classification is made at the leaves.\\nStudent: Can you show me an example exam problem?\\nTeaching Assistant: **Example Exam Problem: Classification Trees**\\n\\n**Problem Description:**\\nYou are provided with a dataset of 15 animals, each characterized by four binary features: Cold Blooded (x1), Has Legs (x2), Lay Eggs (x3), and Has Fur (x4). The goal is to predict whether an animal is a Mammal (y) or not based on these features.\\n\\n**Dataset:**\\n- Snake: (yes, no, yes, no)\\n- Starfish: (yes, no, yes, no)\\n- Bluebird: (no, yes, yes, no)\\n- Blackbird: (no, yes, yes, no)\\n- Earthworm: (yes, no, yes, no)\\n- Chameleon: (yes, no, yes, no)\\n- Ant: (yes, no, yes, no)\\n- Jellyfish: (yes, no, yes, no)\\n- Snail: (yes, no, yes, no)\\n- Sea Urchin: (yes, no, yes, no)\\n- Dolphin: (no, yes, no, yes)\\n- Rat: (no, yes, no, yes)\\n- Dog: (no, yes, no, yes)\\n- Monkey: (no, yes, no, yes)\\n- Lion: (no, yes, no, yes)\\n\\n**Task:**\\n1. Using the dataset provided, construct a classification tree to predict the 'Mammal' label. Start by choosing a feature that best splits the data at the root, and continue splitting each subset using the remaining features until all nodes are pure or no further splitting is possible.\\n2. Explain the rationale behind each split you choose and how it helps in classifying the animals.\\n3. Draw the resulting classification tree.\\n\\n**Hints:**\\n- Consider the purity of nodes after each split. A node is pure if all its instances belong to a single class (either all mammals or all non-mammals).\\n- Use binary questions (yes/no) based on the features to split the data at each node.\\n\\nThis problem will help you understand how to manually build a classification tree and how to use it for making predictions based on the features of new instances.\\nStudent: How would I implement this in Python?\\nTeaching Assistant: To assist you best, could you please specify what you would like to implement in Python? Are you looking to create a classification tree for the example exam problem we discussed, or is there another specific aspect of classification trees or machine learning that you need help implementing? This will help me provide you with the most relevant code and explanation.\\nStudent: For the example exam problem\\nTeaching Assistant: Here's a Python implementation to solve the classification tree problem for the given dataset:\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\nimport matplotlib.pyplot as plt\\n\\n# Create the dataset\\ndata = {\\n    'Cold Blooded': [1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\\n    'Has Legs': [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\\n    'Lay Eggs': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\\n    'Has Fur': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\\n    'Mammal': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\\n}\\ndf = pd.DataFrame(data)\\n\\n# Features and target\\nX = df[['Cold Blooded', 'Has Legs', 'Lay Eggs', 'Has Fur']]\\ny = df['Mammal']\\n\\n# Create and train the classifier\\nclassifier = DecisionTreeClassifier(criterion='entropy')\\nclassifier.fit(X, y)\\n\\n# Visualize the tree\\nplt.figure(figsize=(12,8))\\ntree.plot_tree(classifier, filled=True, feature_names=['Cold Blooded', 'Has Legs', 'Lay Eggs', 'Has Fur'], class_names=['Not Mammal', 'Mammal'])\\nplt.show()\\n```\\n\\nThis code will:\\n1. Create a DataFrame with the given dataset.\\n2. Separate the features and the target label.\\n3. Train a Decision Tree Classifier using the entropy criterion to manage splits based on information gain.\\n4. Visualize the tree to see how the features split the data and how decisions are made at each node.\\n\\nThis visualization will help you understand the structure of the classification tree and how it classifies the animals based on their features.\\nStudent: Can you explain what is meant by entropy in the plot? and how this is computed\\nTeaching Assistant: In the context of decision trees, entropy is used to measure the impurity or randomness in the dataset at each node. It is computed using the formula $H(S) = -\\\\sum_{i=1}^{n} p_i \\\\log_2 p_i$, where $p_i$ represents the proportion of class $i$ instances at the node. The aim in using entropy is to choose the splits that result in the most significant decrease in entropy, leading to branches with higher purity. This method helps in building an efficient and effective classification tree.\\nStudent: Ahh, so in the plot, the largest decrease in entropy happens when each animal is categorized on whether they have fur or not?\\nTeaching Assistant: Yes, in the plot of a decision tree for classifying animals, the feature 'Has Fur' can result in the largest decrease in entropy when used to categorize animals, especially if it distinctly separates mammals from non-mammals. This significant reduction in entropy is due to the feature creating highly pure (homogeneous) branches, which is ideal for effective classification.\\nStudent: Ok I think I understand it now\\nTeaching Assistant: That's great to hear, August! If you feel comfortable with the topic of classification trees and their implementation, do you have any other questions or is there another topic within machine learning that you would like to explore? I'm here to help with any aspect of your studies!\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe conversation between August and the Teaching Assistant provides some insights into August's learning preferences, particularly regarding the application of theoretical concepts through practical examples and coding.\n",
      "\n",
      "Thought:\n",
      "From the dialogue, it's clear that August prefers learning through examples and practical implementation. This is evident from his request for an example exam problem and subsequent interest in implementing the solution in Python. Additionally, his query about entropy and its role in decision trees suggests a preference for understanding theoretical concepts through their practical applications in coding.\n",
      "\n",
      "Action:\n",
      "No further action is required as the analysis is based on the provided conversation.\n",
      "\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "\n",
      "Final Answer:\n",
      "{\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Practical application of concepts\", \"Visual learning through examples\", \"Step-by-step explanation\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you show me an example exam problem?\",\n",
      "    \"Student: How would I implement this in Python?\",\n",
      "    \"Student: Can you explain what is meant by entropy in the plot? and how this is computed\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is the response from the model:\n",
      " {\n",
      "  \"found_preferences\": true,\n",
      "  \"learning_preferences\": [\"Practical application of concepts\", \"Visual learning through examples\", \"Step-by-step explanation\"],\n",
      "  \"indications\": [\n",
      "    \"Student: Can you show me an example exam problem?\",\n",
      "    \"Student: How would I implement this in Python?\",\n",
      "    \"Student: Can you explain what is meant by entropy in the plot? and how this is computed\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_conversation = \"Student: Hello, I am August, studying at the technical university of denmark, in the course Introduction to Machine Learning\\nTeaching Assistant: Hi August! Welcome to the course on Introduction to Machine Learning. How can I assist you today? Are there specific topics or concepts within machine learning that you would like to discuss or need help understanding?\\nStudent: I am studying for the written exam, in which I have to be able to apply some of the methods to problems. Right now I am having trouble with understanding classification trees, specifically how I can compute an output from these in hand\\nTeaching Assistant: To compute an output from a classification tree by hand, you follow these steps:\\n\\n1. **Start at the Root Node**: Begin at the top of the tree. This node contains a condition or question about one of the features of your data. For instance, in the provided dataset, a question at the root might be \\\"Does the animal have fur?\\\"\\n\\n2. **Follow the Branches Based on Features**: Depending on the answer to the question (Yes or No), follow the branch down to the next node. Each node in the tree represents another question about a feature until you reach a leaf node.\\n\\n3. **Reach a Leaf Node**: Leaf nodes represent the classification outcome based on the path of decisions made from the root to that leaf. For example, if you followed the path for an animal that has fur, you might end up at a leaf node classifying the animal as a mammal.\\n\\n4. **Example Calculation**:\\n   - Consider a new example, say a 'Bear' with features: Cold Blooded (no), Has Legs (yes), Lay Eggs (no), Has Fur (yes).\\n   - Starting at the root, if the first question is about having fur, you would answer yes and follow the branch to the next node.\\n   - If the subsequent nodes ask about laying eggs or being cold-blooded, your answers (no, no) will guide you through specific branches.\\n   - Finally, you would reach a leaf node that classifies the bear as a mammal based on the answers provided.\\n\\nThis method of traversing from the root to a leaf, based on feature responses, is how you manually compute an output from a classification tree. Each decision point (node) splits the data further based on the feature until a final classification is made at the leaves.\\nStudent: Can you show me an example exam problem?\\nTeaching Assistant: **Example Exam Problem: Classification Trees**\\n\\n**Problem Description:**\\nYou are provided with a dataset of 15 animals, each characterized by four binary features: Cold Blooded (x1), Has Legs (x2), Lay Eggs (x3), and Has Fur (x4). The goal is to predict whether an animal is a Mammal (y) or not based on these features.\\n\\n**Dataset:**\\n- Snake: (yes, no, yes, no)\\n- Starfish: (yes, no, yes, no)\\n- Bluebird: (no, yes, yes, no)\\n- Blackbird: (no, yes, yes, no)\\n- Earthworm: (yes, no, yes, no)\\n- Chameleon: (yes, no, yes, no)\\n- Ant: (yes, no, yes, no)\\n- Jellyfish: (yes, no, yes, no)\\n- Snail: (yes, no, yes, no)\\n- Sea Urchin: (yes, no, yes, no)\\n- Dolphin: (no, yes, no, yes)\\n- Rat: (no, yes, no, yes)\\n- Dog: (no, yes, no, yes)\\n- Monkey: (no, yes, no, yes)\\n- Lion: (no, yes, no, yes)\\n\\n**Task:**\\n1. Using the dataset provided, construct a classification tree to predict the 'Mammal' label. Start by choosing a feature that best splits the data at the root, and continue splitting each subset using the remaining features until all nodes are pure or no further splitting is possible.\\n2. Explain the rationale behind each split you choose and how it helps in classifying the animals.\\n3. Draw the resulting classification tree.\\n\\n**Hints:**\\n- Consider the purity of nodes after each split. A node is pure if all its instances belong to a single class (either all mammals or all non-mammals).\\n- Use binary questions (yes/no) based on the features to split the data at each node.\\n\\nThis problem will help you understand how to manually build a classification tree and how to use it for making predictions based on the features of new instances.\\nStudent: How would I implement this in Python?\\nTeaching Assistant: To assist you best, could you please specify what you would like to implement in Python? Are you looking to create a classification tree for the example exam problem we discussed, or is there another specific aspect of classification trees or machine learning that you need help implementing? This will help me provide you with the most relevant code and explanation.\\nStudent: For the example exam problem\\nTeaching Assistant: Here's a Python implementation to solve the classification tree problem for the given dataset:\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\nimport matplotlib.pyplot as plt\\n\\n# Create the dataset\\ndata = {\\n    'Cold Blooded': [1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\\n    'Has Legs': [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\\n    'Lay Eggs': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\\n    'Has Fur': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\\n    'Mammal': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\\n}\\ndf = pd.DataFrame(data)\\n\\n# Features and target\\nX = df[['Cold Blooded', 'Has Legs', 'Lay Eggs', 'Has Fur']]\\ny = df['Mammal']\\n\\n# Create and train the classifier\\nclassifier = DecisionTreeClassifier(criterion='entropy')\\nclassifier.fit(X, y)\\n\\n# Visualize the tree\\nplt.figure(figsize=(12,8))\\ntree.plot_tree(classifier, filled=True, feature_names=['Cold Blooded', 'Has Legs', 'Lay Eggs', 'Has Fur'], class_names=['Not Mammal', 'Mammal'])\\nplt.show()\\n```\\n\\nThis code will:\\n1. Create a DataFrame with the given dataset.\\n2. Separate the features and the target label.\\n3. Train a Decision Tree Classifier using the entropy criterion to manage splits based on information gain.\\n4. Visualize the tree to see how the features split the data and how decisions are made at each node.\\n\\nThis visualization will help you understand the structure of the classification tree and how it classifies the animals based on their features.\\nStudent: Can you explain what is meant by entropy in the plot? and how this is computed\\nTeaching Assistant: In the context of decision trees, entropy is used to measure the impurity or randomness in the dataset at each node. It is computed using the formula $H(S) = -\\\\sum_{i=1}^{n} p_i \\\\log_2 p_i$, where $p_i$ represents the proportion of class $i$ instances at the node. The aim in using entropy is to choose the splits that result in the most significant decrease in entropy, leading to branches with higher purity. This method helps in building an efficient and effective classification tree.\\nStudent: Ahh, so in the plot, the largest decrease in entropy happens when each animal is categorized on whether they have fur or not?\\nTeaching Assistant: Yes, in the plot of a decision tree for classifying animals, the feature 'Has Fur' can result in the largest decrease in entropy when used to categorize animals, especially if it distinctly separates mammals from non-mammals. This significant reduction in entropy is due to the feature creating highly pure (homogeneous) branches, which is ideal for effective classification.\\nStudent: Ok I think I understand it now\\nTeaching Assistant: That's great to hear, August! If you feel comfortable with the topic of classification trees and their implementation, do you have any other questions or is there another topic within machine learning that you would like to explore? I'm here to help with any aspect of your studies!\"\n",
    "\n",
    "student_name = \"August\"\n",
    "student_course = \"IntroToMachineLearning\"\n",
    "student_subject = \"Linear Regression\"\n",
    "\n",
    "llm_model = init_llm_langsmith(llm_key=4, temp=0.5, langsmith_name=\"PMAS Agent Testing 1\")\n",
    "pmas = PMAS(llm_model=llm_model,\n",
    "        course=student_course,\n",
    "        subject=student_subject,\n",
    "        student_name=student_name,\n",
    "        )\n",
    "\n",
    "res = pmas.predict(conversation=test_conversation)\n",
    "print(\"This is the response from the model:\\n\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging the Long Term Memory Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:\n",
      " {'id': '8091e3a1-6209-42f1-ac2b-81a94ea27bd8', 'name': 'AugustSemrau_PMAS_Test1-8091'}\n",
      "\n",
      "Current thread:  a556c772-57c3-4674-b279-d06328092312\n",
      "Past threads:  ['274c778d-421a-4572-abe6-cb39cf3ec52c', '7d326584-c235-4be2-ac66-784699cc25d1', '3c068f9f-e601-4bb0-8927-5ba7aecef5a6', 'd43cd194-0fb0-4075-a257-0be0505b2b83', 'f8df743a-893d-4bf1-bec5-5962cfbb3292', 'ae58788c-cd47-4a79-88e5-de10bbb89602', 'ed48ce37-b338-4247-bab8-500128767511', '6a966685-e760-41cb-b9dd-356a15ba6ba6', 'fe73add6-f1f9-4796-befe-4a7681427a4f', '28421c1b-4ab9-4b7e-aa81-24b49cafb598', 'fd7c676c-a221-428d-a2ab-4d028a6ee305', 'f9da97a0-d04a-476d-a8c1-a377541c2a55', 'b24cccca-10cf-4828-9196-fc0a90e10f0a', '02261128-1763-4680-9206-5d5af44bb489', 'e5e82bae-3a50-483c-9376-183641056df9', 'bb093021-7f3b-4342-874a-fa513f71ad79', 'f8217c6e-b924-42a1-b74e-73be9d3647e4', 'e21a25dd-10da-4103-9700-5510445e0574', 'a86c37bf-06a9-4854-a699-1af2c3c4fc9a', '983b3d4e-0346-4577-8871-f72e237dd828', '0f9f67ac-ac30-402f-91ee-3210fb599e7a']\n",
      "\n",
      "Memory Functions: \n",
      "{'id': 'f9a97c15-b2c8-43f7-9c61-1db658173f1d', 'type': 'user_semantic_memory', 'custom_instructions': None, 'schema': {'name': 'SemanticUserMemory', 'description': 'Remember key information about the user based on the conversation.', 'parameters': {'title': 'extractTriplet', 'description': 'Extract RDF triples from the conversation.', 'type': 'object', 'properties': {'triples': {'title': 'Triples', 'description': 'The facts to extract as memories for future conversations. Be as detailed as possible, while focusing on the user information. Use names over IDs where possible.', 'type': 'array', 'items': {'$ref': '#/definitions/Triple'}}}, 'required': ['triples'], 'definitions': {'Triple': {'title': 'Triple', 'description': 'RDF triple to extract.', 'type': 'object', 'properties': {'subject': {'title': 'Subject', 'type': 'string'}, 'predicate': {'title': 'Predicate', 'type': 'string'}, 'object': {'title': 'Object', 'type': 'string'}, 'importance': {'title': 'Importance', 'description': \"Absolute importance of this memory or fact about the user,  with 1 being mundane information and 10 being poignant, identity-shaping memories about the user that you mustn't forget.\", 'minimum': 1, 'maximum': 10, 'type': 'integer'}}, 'required': ['subject', 'predicate', 'object', 'importance']}}}}, 'status': 'active', 'target_type': 'user'}\n",
      "{'id': '31a96c50-bff8-4e02-85d5-27f9626399d9', 'type': 'user_state', 'custom_instructions': None, 'schema': {'name': 'UserProfile', 'description': \"A user's profile.\", 'parameters': {'properties': {'student_name': {'default': None, 'description': 'The name of the student.', 'title': 'Student Name', 'type': 'string'}, 'summary': {'default': '', 'description': 'A quick summary of how the user would describe themselves.', 'title': 'Summary', 'type': 'string'}, 'education': {'default': None, 'description': 'The education which the studernt undertakes.', 'title': 'Education', 'type': 'string'}, 'courses': {'description': 'The courses the student is currently taking.', 'items': {'type': 'string'}, 'title': 'Courses', 'type': 'array'}, 'learning_preferences': {'default': '', 'description': 'The learning preferences of the student.', 'title': 'Learning Preferences', 'type': 'string'}}, 'type': 'object'}}, 'status': 'active', 'target_type': 'user'}\n",
      "{'id': '51b551ca-8b86-4039-a5f8-c85da12119b9', 'type': 'user_append_state', 'custom_instructions': 'Identify what learning preferences the student may have, which are apparent in the conversation.', 'schema': {'name': 'LearningPreference', 'description': 'A learning preference of the user.', 'parameters': {'properties': {'learning_preference': {'default': '', 'description': 'The learning preference.', 'title': 'Learning Preference', 'type': 'string'}, 'why': {'default': '', 'description': 'Why the user prefers this way of teaching.', 'title': 'Why', 'type': 'string'}, 'context': {'default': '', 'description': 'The context and perhaps courses/subjects in which this learning prefernce is apparent.', 'title': 'Context', 'type': 'string'}, 'source_comment': {'default': '', 'description': 'The raw user utterance where you identified this preference.', 'title': 'Source Comment', 'type': 'string'}}, 'type': 'object'}}, 'status': 'active', 'target_type': 'user'}\n",
      "{'id': '5ad29814-92fa-45ef-9086-71f3e508005d', 'type': 'user_append_state', 'custom_instructions': 'Extract the subjects delved with in the conversation.', 'schema': {'name': 'SubjectComprehension', 'description': 'Formative events for the user.', 'parameters': {'properties': {'subject': {'default': '', 'description': 'The subject which the student discussed in this interaction/conversation.', 'title': 'Subject', 'type': 'string'}, 'comprehension': {'default': '', 'description': 'How well did the user comprehend this subject? Did they show clear comprehension, a lack of comprehension, or not show whether their comprehend the subject?', 'title': 'Comprehension', 'type': 'string'}}, 'type': 'object'}}, 'status': 'active', 'target_type': 'user'}\n",
      "{'id': '22a69cf4-efc4-4304-9694-38599bf29614', 'type': 'thread_summary', 'custom_instructions': None, 'schema': {'name': 'ConversationSummary', 'description': 'Summary of a conversation.', 'parameters': {'properties': {'title': {'default': '', 'description': 'Concise 2-5 word title for conversation.', 'title': 'Title', 'type': 'string'}, 'summary': {'default': '', 'description': 'High level summary of the interactions.', 'title': 'Summary', 'type': 'string'}, 'topic': {'description': 'Tags for topics discussed in this conversation.', 'items': {'type': 'string'}, 'title': 'Topic', 'type': 'array'}}, 'type': 'object'}}, 'status': 'active', 'target_type': 'thread'}\n",
      "Thread id 3c068f9f-e601-4bb0-8927-5ba7aecef5a6 has the following summary: {'title': 'Linear Regression Explained', 'topic': ['Machine Learning', 'Linear Regression', 'Education'], 'summary': 'The conversation revolves around explaining linear regression to a user named August, who is learning about it for their Intro to Machine Learning course. The AI provides detailed explanations on linear regression, including its basic concept, formulas, applications, and a Python code example. August seeks clarification on various aspects, such as the formulas relevant for exams, alternative methods for estimating coefficients, and the L2 regularization formula. The AI also crafts a trial exam question for August, further explaining the components of a multiple linear regression model and how to estimate coefficients, address overfitting, and evaluate model performance. Despite a misunderstanding where August felt an explanation was repeated unnecessarily, the conversation ends on a positive note with August thanking the AI.'}\n",
      "Thread id f9da97a0-d04a-476d-a8c1-a377541c2a55 has the following summary: {'title': 'Understanding Cross-Validation', 'topic': ['Machine Learning', 'Cross-Validation', 'Python Programming'], 'summary': 'The conversation revolves around explaining the concept of cross-validation, a statistical method used in machine learning to evaluate model performance and prevent overfitting. The AI elaborates on different types of cross-validation, including K-fold and leave-one-out, and provides a real-world example of predicting real estate prices, including a Python code snippet using scikit-learn. The user requests a code example without using scikit-learn, leading to a discussion on manually implementing cross-validation and the importance of data shuffling and stratification in preparing data for machine learning models.'}\n",
      "Thread id b24cccca-10cf-4828-9196-fc0a90e10f0a has the following summary: {'title': 'Understanding Ensemble Methods', 'topic': ['Machine Learning', 'Ensemble Methods', 'Bagging', 'Predictive Modeling'], 'summary': 'The conversation involves an AI explaining ensemble methods in machine learning to a user named August. The AI elaborates on the concept of ensemble methods, focusing on bagging and its application in machine learning for improved predictive performance. The AI provides detailed explanations of bagging, including its steps, advantages, and examples of its use in credit risk assessment and weather forecasting. August expresses understanding and gratitude for the information provided.'}\n",
      "Thread id 02261128-1763-4680-9206-5d5af44bb489 has the following summary: {'title': 'Understanding PCA', 'topic': ['PCA', 'Data Simplification', 'Singular Value Decomposition', 'Eigenvalues and Eigenvectors'], 'summary': 'The user requested help with understanding Principal Component Analysis (PCA), starting with a brief introduction. The AI explained PCA as a statistical procedure for simplifying data by transforming correlated variables into a set of linearly uncorrelated variables called principal components. The explanation covered the steps involved in PCA, including standardization, computation of the covariance matrix, calculation of eigenvalues and eigenvectors, and component selection. The user then inquired about the mathematical implementation of eigenvalue calculation, to which the AI described using Singular Value Decomposition (SVD) for efficient and stable computation.'}\n",
      "Thread id e5e82bae-3a50-483c-9376-183641056df9 has the following summary: {'title': 'Understanding PCA', 'topic': ['PCA', 'Data Analysis', 'Machine Learning'], 'summary': 'The user requested help with understanding Principal Component Analysis (PCA), a statistical technique for reducing dataset dimensionality. The AI provided a detailed explanation of PCA, including its steps: standardization, computing the covariance matrix, eigen decomposition, sorting eigenvectors, and projecting the original data onto the principal components. The AI also mentioned the use of PCA in various fields and offered further assistance.'}\n",
      "Thread id f8217c6e-b924-42a1-b74e-73be9d3647e4 has the following summary: {'title': 'Understanding Bayesian Probabilities', 'topic': ['Machine Learning', 'Bayesian Probabilities', \"Bayes' Theorem\"], 'summary': \"The user inquired about probabilities relevant to machine learning, focusing on Bayesian probabilities. The AI provided a detailed explanation of how probabilities are used in machine learning, including Bayesian methods, and elaborated on Bayes' Theorem with an analogy to help the user understand. The user expressed satisfaction with the explanation.\"}\n",
      "Thread id a86c37bf-06a9-4854-a699-1af2c3c4fc9a has the following summary: {'title': 'Understanding Classification Trees', 'topic': ['Machine Learning', 'Classification Trees', 'Python Implementation', 'Entropy'], 'summary': 'August, a student at the Technical University of Denmark, sought help understanding classification trees for a written exam in an Introduction to Machine Learning course. The conversation covered how to compute outputs from classification trees by hand, provided an example exam problem, and discussed implementing a solution in Python. The discussion also touched on the concept of entropy in decision trees. August expressed understanding of the topic by the end of the conversation.'}\n",
      "Thread id 0f9f67ac-ac30-402f-91ee-3210fb599e7a has the following summary: {'title': 'Explaining Transformers', 'topic': ['NLP', 'Transformers', 'Machine Learning'], 'summary': 'The user inquired about Transformers in NLP, and the AI provided a detailed explanation of the architecture, including its attention mechanism, multi-head attention, positional encoding, and feed-forward networks. The AI also mentioned how Transformers process data in parallel, unlike previous sequential models, and highlighted their significance in advancing NLP with models like BERT and GPT.'}\n",
      "\n",
      "These are the thread summaries:\n",
      "\n",
      " {'3c068f9f-e601-4bb0-8927-5ba7aecef5a6': {'title': 'Linear Regression Explained', 'topic': ['Machine Learning', 'Linear Regression', 'Education'], 'summary': 'The conversation revolves around explaining linear regression to a user named August, who is learning about it for their Intro to Machine Learning course. The AI provides detailed explanations on linear regression, including its basic concept, formulas, applications, and a Python code example. August seeks clarification on various aspects, such as the formulas relevant for exams, alternative methods for estimating coefficients, and the L2 regularization formula. The AI also crafts a trial exam question for August, further explaining the components of a multiple linear regression model and how to estimate coefficients, address overfitting, and evaluate model performance. Despite a misunderstanding where August felt an explanation was repeated unnecessarily, the conversation ends on a positive note with August thanking the AI.'}, 'f9da97a0-d04a-476d-a8c1-a377541c2a55': {'title': 'Understanding Cross-Validation', 'topic': ['Machine Learning', 'Cross-Validation', 'Python Programming'], 'summary': 'The conversation revolves around explaining the concept of cross-validation, a statistical method used in machine learning to evaluate model performance and prevent overfitting. The AI elaborates on different types of cross-validation, including K-fold and leave-one-out, and provides a real-world example of predicting real estate prices, including a Python code snippet using scikit-learn. The user requests a code example without using scikit-learn, leading to a discussion on manually implementing cross-validation and the importance of data shuffling and stratification in preparing data for machine learning models.'}, 'b24cccca-10cf-4828-9196-fc0a90e10f0a': {'title': 'Understanding Ensemble Methods', 'topic': ['Machine Learning', 'Ensemble Methods', 'Bagging', 'Predictive Modeling'], 'summary': 'The conversation involves an AI explaining ensemble methods in machine learning to a user named August. The AI elaborates on the concept of ensemble methods, focusing on bagging and its application in machine learning for improved predictive performance. The AI provides detailed explanations of bagging, including its steps, advantages, and examples of its use in credit risk assessment and weather forecasting. August expresses understanding and gratitude for the information provided.'}, '02261128-1763-4680-9206-5d5af44bb489': {'title': 'Understanding PCA', 'topic': ['PCA', 'Data Simplification', 'Singular Value Decomposition', 'Eigenvalues and Eigenvectors'], 'summary': 'The user requested help with understanding Principal Component Analysis (PCA), starting with a brief introduction. The AI explained PCA as a statistical procedure for simplifying data by transforming correlated variables into a set of linearly uncorrelated variables called principal components. The explanation covered the steps involved in PCA, including standardization, computation of the covariance matrix, calculation of eigenvalues and eigenvectors, and component selection. The user then inquired about the mathematical implementation of eigenvalue calculation, to which the AI described using Singular Value Decomposition (SVD) for efficient and stable computation.'}, 'e5e82bae-3a50-483c-9376-183641056df9': {'title': 'Understanding PCA', 'topic': ['PCA', 'Data Analysis', 'Machine Learning'], 'summary': 'The user requested help with understanding Principal Component Analysis (PCA), a statistical technique for reducing dataset dimensionality. The AI provided a detailed explanation of PCA, including its steps: standardization, computing the covariance matrix, eigen decomposition, sorting eigenvectors, and projecting the original data onto the principal components. The AI also mentioned the use of PCA in various fields and offered further assistance.'}, 'f8217c6e-b924-42a1-b74e-73be9d3647e4': {'title': 'Understanding Bayesian Probabilities', 'topic': ['Machine Learning', 'Bayesian Probabilities', \"Bayes' Theorem\"], 'summary': \"The user inquired about probabilities relevant to machine learning, focusing on Bayesian probabilities. The AI provided a detailed explanation of how probabilities are used in machine learning, including Bayesian methods, and elaborated on Bayes' Theorem with an analogy to help the user understand. The user expressed satisfaction with the explanation.\"}, 'a86c37bf-06a9-4854-a699-1af2c3c4fc9a': {'title': 'Understanding Classification Trees', 'topic': ['Machine Learning', 'Classification Trees', 'Python Implementation', 'Entropy'], 'summary': 'August, a student at the Technical University of Denmark, sought help understanding classification trees for a written exam in an Introduction to Machine Learning course. The conversation covered how to compute outputs from classification trees by hand, provided an example exam problem, and discussed implementing a solution in Python. The discussion also touched on the concept of entropy in decision trees. August expressed understanding of the topic by the end of the conversation.'}, '0f9f67ac-ac30-402f-91ee-3210fb599e7a': {'title': 'Explaining Transformers', 'topic': ['NLP', 'Transformers', 'Machine Learning'], 'summary': 'The user inquired about Transformers in NLP, and the AI provided a detailed explanation of the architecture, including its attention mechanism, multi-head attention, positional encoding, and feed-forward networks. The AI also mentioned how Transformers process data in parallel, unlike previous sequential models, and highlighted their significance in advancing NLP with models like BERT and GPT.'}}\n"
     ]
    }
   ],
   "source": [
    "from thesis2024.utils import init_llm_langsmith\n",
    "from thesis2024.PMAS import LongTermMemory\n",
    "langsmith_name = \"LangMem TEST 1 \"\n",
    "llm_model = init_llm_langsmith(llm_key=40, temp=0.5, langsmith_name=langsmith_name)\n",
    "\n",
    "ltm_class = LongTermMemory(user_name=\"AugustSemrau_PMAS_Test1\")\n",
    "print(\"User:\\n\", ltm_class.get_user_data())\n",
    "print(\"\\nCurrent thread: \", ltm_class.thread_id)\n",
    "past_threads = ltm_class.past_thread_ids\n",
    "print(\"Past threads: \", past_threads)\n",
    "\n",
    "functions = ltm_class.client.list_memory_functions()\n",
    "print(\"\\nMemory Functions: \")\n",
    "for function in functions:\n",
    "    print(function)\n",
    "\n",
    "summaries = ltm_class.get_thread_summaries()\n",
    "print(\"\\nThese are the thread summaries:\\n\\n\", summaries)\n",
    "\n",
    "# print(\"\\nPast Thread Memories:\\n\")\n",
    "# for thread in past_threads:\n",
    "#     print(thread)\n",
    "#     print(ltm_class.client.list_thread_memory(thread_id=str(thread))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Student Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "User Profile:  {'courses': ['Probabilities'], 'education': 'Machine Learning'}\n",
      "User state:\n",
      "\n",
      " {'courses': ['Probabilities'], 'education': 'Machine Learning'}\n"
     ]
    }
   ],
   "source": [
    "user_state = ltm_class.get_user_state()\n",
    "print(\"User state:\\n\\n\", user_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Semantic Memories for the Student in all threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the memories:\n",
      "\n",
      " AugustSemrau_PMAS_Test1-8091 is called August.\n",
      "August needs simplified explanations.\n",
      "August benefits from simplified explanations.\n",
      "August interested in methods like linear regression.\n",
      "August wants to learn about Transformers in NLP.\n",
      "August interested in ensemble methods.\n",
      "August requested example exam problem.\n",
      "August prefers concise explanations.\n",
      "August interested in bagging further explanation.\n",
      "August needs trial exam questions for practice\n"
     ]
    }
   ],
   "source": [
    "memories = ltm_class.get_user_semantic_memories(query=\"\")\n",
    "print(\"These are the memories:\\n\\n\",memories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get student learning preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Learning preference memories:  {'user_id': '8091e3a1-6209-42f1-ac2b-81a94ea27bd8', 'memories': [], 'state': None, 'thread_summaries': None}\n",
      "These are the learning preferences:\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "learning_preferences = ltm_class.get_learning_preference_memories()\n",
    "print(\"These are the learning preferences:\\n\\n\", learning_preferences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student subject comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Subject comprehension memories:  {'user_id': '8091e3a1-6209-42f1-ac2b-81a94ea27bd8', 'memories': [], 'state': None, 'thread_summaries': None}\n",
      "These are the subject comprehension memories:\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "subject_comprehension = ltm_class.get_subject_comprehension_memories()\n",
    "print(\"These are the subject comprehension memories:\\n\\n\", subject_comprehension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset memory functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleanup\n",
    "functions = ltm_class.client.list_memory_functions()\n",
    "\n",
    "for func in functions:\n",
    "    if func[\"type\"] == \"user_semantic_memory\":\n",
    "        continue\n",
    "    ltm_class.client.delete_memory_function(func[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redudnant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm_class.save_conversation_step(user_query=\"Hello I am August and I study computer science at the Technical University of Denmark. I come from Jutland in Denmark. I do not understand math, and NEED to be explained technical concepts in words, not formulas.\", llm_response=\"Hello August, how can I assist you today?\")\n",
    "ltm_class.save_conversation_step(user_query=\"I am studying the course IntroToMachineLearning and am trying to learn about the subject Cross-validation.\\nI am having a hard time understanding the course material revolving around this subject. Can you help me understand it better by rewriting it to my learning preferences, which are the following:\\nI prefer text-based explanations and metaphors in order to understand technical concepts?\", \n",
    "                                 llm_response=\"Certainly, August! Let's dive into the concept of cross-validation using a metaphor that might help clarify things.\\n\\nImagine you're a chef trying to perfect a new recipe. Before you serve it at a big dinner party, you want to make sure it tastes just right and appeals to a variety of palates. To do this, you could cook the dish several times, each time using a different group of friends as taste-testers. Each group provides feedback, and you tweak the recipe accordingly. This way, you ensure the final version will be well-received by a diverse crowd.\\n\\nIn machine learning, cross-validation works in a similar way. When you build a model, you want it to perform well not just on the data you used to create it, but on new, unseen data as well. Cross-validation helps you ensure this by dividing your data into several smaller sets or \\\"folds\\\". Here's how it typically works:\\n\\n1. **Splitting the Data**: Just like dividing your friends into different tasting groups, you split your dataset into, say, 5 parts (or any other number, commonly referred to as \\\"k\\\"). Each part is called a fold.\\n\\n2. **Training and Validating**: You then train your model on 4 of these folds, using them as your training data (like cooking for four groups of friends). The remaining fold serves as your validation data, which is like the fifth group of friends you haven't cooked for yet. This group is used to evaluate how well your model performs.\\n\\n3. **Repeating the Process**: Just as you would rotate the role of taste-tester among your groups of friends, you rotate the validation fold in cross-validation. Each of the 5 folds gets a turn to be the validation set, with the model being trained on the other 4 folds each time.\\n\\n4. **Averaging the Results**: In the end, you average the performance metrics (like accuracy) from each of the 5 trials. This gives you a good idea of how well your model is likely to perform on unseen data.\\n\\nThis process helps prevent a common problem in machine learning called \\\"overfitting,\\\" where a model performs well on its training data but poorly on new, unseen data. It's like making sure your dish doesn’t just appeal to one group of friends but to all kinds of guests.\\n\\nI hope this metaphor helps you understand the concept of cross-validation better! If you have any more questions or need further details, feel free to ask.\")\n",
    "\n",
    "ltm_class.save_conversation_step(user_query=\"I think that makes it pretty clear for me! Thank you for the explanation.\",\n",
    "                                 llm_response=\"That's great to hear, August! I'm glad the metaphor helped clarify things for you. If you have any more questions or need further assistance with cross-validation or any other topic, don't hesitate to reach out. I'm here to help!\")\n",
    "\n",
    "ltm_class.save_conversation_step(user_query=\"What is the height of Mount Everest?.\", llm_response=\"I will figure out.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_conversation = (\"Student: Hello \\n\"\n",
    "    \" Teacher: Hello, how are you? \\n\"\n",
    "    \" Student: Good, but can you help me understand what gradient descent is? \\n\"\n",
    "    \" Teacher: Sure, I can help you with that. Gradient descent is an optimization algorithm used to minimize the cost function in machine learning. It works by iteratively updating the parameters of a model in the direction of the negative gradient of the cost function. This process is repeated until the algorithm converges to a local minimum of the cost function. Does that help? \\n\"\n",
    "    \" Student: Yes, thank you for explaining that. Can you also explain how gradient descent is used in training neural networks? \\n\"\n",
    "    \" Teacher: Of course! In training neural networks, gradient descent is used to update the weights and biases of the network in order to minimize the loss function. The gradient of the loss function with respect to the weights and biases is calculated using backpropagation, and then the weights and biases are updated in the direction of the negative gradient. This process is repeated for each batch of training data until the network converges to a set of weights and biases that minimize the loss function. Does that make sense? \\n\"\n",
    "    \" Student: Yes, that makes sense. Thank you for explaining it so clearly. \\n\"\n",
    "    \" Teacher: You're welcome! If you have any more questions, feel free to ask. I'm here to help. \\n\"\n",
    "    \" Student: Thank you, I will keep that in mind.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
