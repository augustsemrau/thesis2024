{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import functools\n",
    "import operator\n",
    "import os\n",
    "import getpass\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "# Langchain packages\n",
    "from langchain import hub\n",
    "from langchain.agents import (AgentExecutor,\n",
    "                              AgentOutputParser,\n",
    "                              AgentType,\n",
    "                              Tool,\n",
    "                              create_react_agent)\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.messages import (AIMessage,\n",
    "                                     BaseMessage,\n",
    "                                     ChatMessage,\n",
    "                                     FunctionMessage,\n",
    "                                     HumanMessage,\n",
    "                                     SystemMessage)\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (PromptTemplate,\n",
    "                               ChatPromptTemplate, \n",
    "                               HumanMessagePromptTemplate,\n",
    "                               MessagesPlaceholder)\n",
    "\n",
    "# Langgraph\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Tools\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "\n",
    "def init_llm_langsmith(llm_key = 3):\n",
    "    \"\"\"Initialize the LLM model for the LangSmith system.\"\"\"\n",
    "    # Set environment variables\n",
    "    def _set_if_undefined(var: str):\n",
    "        if not os.environ.get(var):\n",
    "            os.environ[var] = getpass(f\"Please provide your {var}\")\n",
    "    _set_if_undefined(\"OPENAI_API_KEY\")\n",
    "    _set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "    # Add tracing in LangSmith.\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "    if llm_key == 3:\n",
    "        llm = \"gpt-3.5-turbo-0125\"\n",
    "        os.environ[\"LANGCHAIN_PROJECT\"] = \"GPT-3.5 Teaching Agent System TEST 1\"\n",
    "    elif llm_key == 4:\n",
    "        llm = \"gpt-4-0125-preview\"\n",
    "        os.environ[\"LANGCHAIN_PROJECT\"] = \"GPT-4 Teaching Agent System TEST 1\"\n",
    "    return llm\n",
    "\n",
    "llm_ver = init_llm_langsmith(llm_key=3)\n",
    "llm_model = ChatOpenAI(model_name=llm_ver, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This defines the object that is passed between each node\n",
    "# in the graph. We will create different nodes for each agent and tool\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    sender: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(self, state, agent, name):\n",
    "    \"\"\"Helper function to create a node for a given agent. Node that invokes agent.\"\"\"\n",
    "    result = agent.invoke(state)\n",
    "    # We convert the agent output into a format that is suitable to append to the global state\n",
    "    if isinstance(result, FunctionMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = HumanMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        # Since we have a strict workflow, we can\n",
    "        # track the sender so we know who to pass to next.\n",
    "        \"sender\": name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frontline_node(self, state: BaseMessage):\n",
    "    \"\"\"Create the frontline node.\"\"\"\n",
    "    system_message = \"\"\"You are frontline teaching assistant.\n",
    "                    Be concise in your responses.\n",
    "                    You can chat with students and help them with basic questions, but if the student has a teaching-related question,\n",
    "                    do not try to answer the question directly or gather information.\n",
    "                    Instead, immediately transfer them to the teaching agent system by asking the user to hold for a moment.\n",
    "                    Otherwise, just respond conversationally.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system_message),\n",
    "                                                MessagesPlaceholder(variable_name=\"messages\")])\n",
    "    chain = prompt | self.model\n",
    "    node = functools.partial(self.agent_node, agent=chain, name=\"Frontline\")\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Teaching Agent System Graph.\"\"\"\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "frontline_node = create_frontline_node()\n",
    "graph.add_node(frontline_node)\n",
    "graph.set_entry_point(frontline_node)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tools.\"\"\"\n",
    "search = DuckDuckGoSearchAPIWrapper()\n",
    "search_tool = Tool(name=\"Current Search\",\n",
    "                   func=search.run,\n",
    "                   description=\"Useful when you need to answer questions about nouns, current events or the current state of the world.\"\n",
    "                   )\n",
    "\n",
    "# Gradient descent code\n",
    "def coding_function(query: str):\n",
    "    return \"\"\"This is a coding example.\n",
    "\n",
    "    ```python\n",
    "    def gradient_descent(x, y, learning_rate=0.01, epochs=100):\n",
    "        m = 0\n",
    "        b = 0\n",
    "        n = len(x)\n",
    "        for _ in range(epochs):\n",
    "            y_pred = m * x + b\n",
    "            m_gradient = (-2/n) * sum(x * (y - y_pred))\n",
    "            b_gradient = (-2/n) * sum(y - y_pred)\n",
    "            m -= learning_rate * m_gradient\n",
    "            b -= learning_rate * b_gradient\n",
    "        return m, b\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "coding_tool = StructuredTool.from_function(\n",
    "                            func=coding_function,\n",
    "                            name=\"Coding Tool\",\n",
    "                            description=\"Useful when you need to answer questions using a coded example.\"\n",
    "                            )\n",
    "\n",
    "\n",
    "tools = [search_tool, coding_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Teaching Agents System with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'chat_history', 'input', 'tool_names', 'tools'] partial_variables={'system_message': 'Teaching Assistant is designed to be able to assist with teaching related tasks, from answering simple questions to providing in-depth explanations and discussions on STEM topics. \\nAs a language model, Teaching Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nTeaching Assistant is constantly learning and improving, and its capabilities are constantly evolving. \\nIt is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. \\nAdditionally, Teaching Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Teaching Assistant is a powerful tool that can help students with understanding STEM topics. \\nWhether you need help with a specific question or just want to have a conversation about a particular topic, Teaching Assistant is here to assist.\\n', 'course_name': 'Introduction to Computer Science', 'subject_name': 'Gradient Descent'} template='Teaching Assistant is a large language model trained by OpenAI.\\n\\n{system_message}\\n\\nTeaching Assistant is assisting in the course {course_name}.\\nIn this particular conversation, the student needs help with learning the subject {subject_name}\\n\\n\\nTOOLS:\\n------\\n\\nTeaching assistant has access to the following tools:\\n\\n{tools}\\n\\nIf you need to use a tool, your final answer MUST be identical to the output of the tool, and you can only call a tool once.\\nIf you use a tool, you MUST use the following format:\\n\\n```\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: an explanation for the task that the tool should solve\\nObservation: the result of the action\\nFinal Answer: [Observation]\\n```\\n\\n\\nIf you do not need to use a tool, you MUST use the format:\\n\\n```\\nThought: Do I need to use a tool? No\\nFinal Answer: [your response here]\\n```\\n\\nBegin!\\n\\nPrevious conversation history:\\n{chat_history}\\n\\nNew input: {input}\\n{agent_scratchpad}'\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "SYSTEM_MESSAGE = \"\"\"Teaching Assistant is designed to be able to assist with teaching related tasks, from answering simple questions to providing in-depth explanations and discussions on STEM topics. \n",
    "As a language model, Teaching Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Teaching Assistant is constantly learning and improving, and its capabilities are constantly evolving. \n",
    "It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. \n",
    "Additionally, Teaching Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Overall, Teaching Assistant is a powerful tool that can help students with understanding STEM topics. \n",
    "Whether you need help with a specific question or just want to have a conversation about a particular topic, Teaching Assistant is here to assist.\n",
    "\"\"\"\n",
    "\n",
    "COURSE = \"Introduction to Computer Science\"\n",
    "SUBJECT = \"Gradient Descent\"\n",
    "\n",
    "prompt_hub_template = hub.pull(\"augustsemrau/react-teaching-chat\").template\n",
    "prompt_template = PromptTemplate.from_template(template=prompt_hub_template)\n",
    "prompt = prompt_template.partial(system_message=SYSTEM_MESSAGE, course_name=COURSE, subject_name=SUBJECT)\n",
    "print(prompt)\n",
    "\n",
    "agent = create_react_agent(llm=llm_model, tools=tools, prompt=prompt, output_parser=None)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=False, handle_parsing_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello August! How can I assist you today with learning about Gradient Descent in Computer Science?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Hello I am August?\",})[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gradient Descent is an optimization algorithm used in machine learning and deep learning to minimize the loss function by iteratively moving towards the minimum of the function. It is commonly used in training models to find the optimal parameters that best fit the data.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is it about?\",})[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "Final Answer: Sure! An analogy for gradient descent is like trying to find the lowest point in a hilly terrain by taking small steps downhill in the steepest direction. You keep adjusting your steps until you reach the lowest point.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Can you give an analogy for that?',\n",
       " 'chat_history': [HumanMessage(content='Hello I am August?'),\n",
       "  AIMessage(content='Hello August! How can I assist you today?'),\n",
       "  HumanMessage(content='I want to know what gradient descent is'),\n",
       "  AIMessage(content='Gradient descent is an optimization algorithm used in machine learning to minimize the loss function by iteratively moving in the direction of the steepest descent of the loss function. It is commonly used in training machine learning models, such as neural networks, to find the optimal parameters that minimize the error between the predicted output and the actual output.'),\n",
       "  HumanMessage(content='Please code an example of gradient descent for me.'),\n",
       "  AIMessage(content='Here is an example of gradient descent algorithm:\\n\\n```python\\n# Initialize variables\\nx = 5\\ny = 10\\n\\n# Calculate the sum\\nz = x + y\\n\\n# Print the result\\nprint(z)\\n```'),\n",
       "  HumanMessage(content='Can you give an analogy for that?'),\n",
       "  AIMessage(content='Sure! An analogy for gradient descent is like trying to find the lowest point in a hilly terrain by taking small steps downhill in the steepest direction. You keep adjusting your steps until you reach the lowest point.')],\n",
       " 'output': 'Sure! An analogy for gradient descent is like trying to find the lowest point in a hilly terrain by taking small steps downhill in the steepest direction. You keep adjusting your steps until you reach the lowest point.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Can you give an analogy for that?\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Please code an example of gradient descent for me.',\n",
       " 'chat_history': [HumanMessage(content='Hello I am August?'),\n",
       "  AIMessage(content='Hello August! How can I assist you today with learning about Gradient Descent in Computer Science?'),\n",
       "  HumanMessage(content='What is it about?'),\n",
       "  AIMessage(content='Gradient Descent is an optimization algorithm used in machine learning and deep learning to minimize the loss function by iteratively moving towards the minimum of the function. It is commonly used in training models to find the optimal parameters that best fit the data.'),\n",
       "  HumanMessage(content='Please code an example of gradient descent for me.'),\n",
       "  AIMessage(content='Here is an example of gradient descent in Python:\\n\\n```python\\ndef gradient_descent(x, y, learning_rate=0.01, epochs=100):\\n    m = 0\\n    b = 0\\n    n = len(x)\\n    for _ in range(epochs):\\n        y_pred = m * x + b\\n        m_gradient = (-2/n) * sum(x * (y - y_pred))\\n        b_gradient = (-2/n) * sum(y - y_pred)\\n        m -= learning_rate * m_gradient\\n        b -= learning_rate * b_gradient\\n    return m, b\\n```\\n\\nThis code snippet demonstrates a simple implementation of gradient descent to find the optimal parameters `m` and `b` for a linear regression model.')],\n",
       " 'output': 'Here is an example of gradient descent in Python:\\n\\n```python\\ndef gradient_descent(x, y, learning_rate=0.01, epochs=100):\\n    m = 0\\n    b = 0\\n    n = len(x)\\n    for _ in range(epochs):\\n        y_pred = m * x + b\\n        m_gradient = (-2/n) * sum(x * (y - y_pred))\\n        b_gradient = (-2/n) * sum(y - y_pred)\\n        m -= learning_rate * m_gradient\\n        b -= learning_rate * b_gradient\\n    return m, b\\n```\\n\\nThis code snippet demonstrates a simple implementation of gradient descent to find the optimal parameters `m` and `b` for a linear regression model.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Please code an example of gradient descent for me.\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leftover Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "search_tool = Tool(name=\"Current Search\",\n",
    "                   func=search.run,\n",
    "                   description=\"Useful when you need to answer questions about nouns, current events or the current state of the world.\"\n",
    "                   )\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a chatbot teaching assistant having a conversation with a human.\"),\n",
    "            # The persistent system prompt\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"),\n",
    "            # Where the memory will be stored.\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"),\n",
    "            # Where the human input will injected\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chat_llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "chat_llm_chain.predict(human_input=\"Hi there my name is August Semrau.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
